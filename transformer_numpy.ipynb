{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPo6V7ZErbfe4K+GKeZChjV",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kmalik22/colabs/blob/main/transformer_numpy.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "IxBz_g1wyZ-e"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "from typing import List"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "D_MODEL = 3\n",
        "D_FF = int(D_MODEL * 2)\n",
        "BSZ = 2"
      ],
      "metadata": {
        "id": "T_yJ3Obf29bU"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#class torch.nn.Linear(in_features, out_features, bias=True, device=None, dtype=None)\n",
        "class MyLinear:\n",
        "  def __init__(self, in_features, out_features, bias=True, debug=True):\n",
        "    # create a matrix (in_features, out_features)\n",
        "    # backprop. During forward, store activations in a buffer.\n",
        "    self.in_features = in_features\n",
        "    self.out_features = out_features\n",
        "    self.has_bias = bias\n",
        "    self.weight = np.random.normal(0, 1, size=(in_features, out_features))\n",
        "    self.bias = np.zeros(shape=(out_features))\n",
        "    self.stored_activations: List[np.array] = []\n",
        "    self.weight_grad = None\n",
        "    self.bias_grad = None\n",
        "    self.debug = debug\n",
        "\n",
        "  def forward(self, activations: np.array):\n",
        "    assert len(activations.shape) >= 2\n",
        "    assert activations.shape[-1] == self.in_features\n",
        "    self.stored_activations.append(activations)\n",
        "    if self.debug:\n",
        "       print(f\"MyLinear.forward(), batch={len(self.stored_activations)}\")\n",
        "    return ( (activations @ self.weight) + self.bias)\n",
        "\n",
        "  def backward(self, output_act_grad: np.array) -> np.array:\n",
        "    \"\"\"Computes weight grad internally. Returns input_act_grad\n",
        "    \"\"\"\n",
        "    assert len(self.stored_activations) > 0\n",
        "    input_acts = np.concat(self.stored_activations)\n",
        "    if self.debug:\n",
        "      print(f\"MyLinear.bwd(), batches:{len(self.stored_activations)}, act.shape:{input_acts.shape}\")\n",
        "    # input_acts = (bsz in_features)\n",
        "    # output_act_grad = (bsz out_features)\n",
        "    # wts = (in_features out_features)\n",
        "    # wgrad = (in_features out_features)\n",
        "    # input_act_grad = (bsz in_features)\n",
        "    bsz = input_acts.shape[0]\n",
        "    assert input_acts.shape[1] == self.in_features, f\"{input_acts.shape[1]=} {self.in_features=}\"\n",
        "    assert output_act_grad.shape == (bsz, self.out_features), f\"{output_act_grad.shape=} {bsz}\"\n",
        "    self.weight_grad = input_acts.transpose() @ output_act_grad # (in_features, bsz) @ (bsz, out_features)\n",
        "    self.bias_grad = output_act_grad.sum(axis=0) # (out_features)\n",
        "    return output_act_grad @ self.weight.transpose()\n",
        "\n",
        "  def clear_grads(self):\n",
        "    self.stored_activations = []\n",
        "    self.weight_grad = None\n",
        "    self.bias_grad = None\n",
        "\n",
        "\n",
        "def make_wts_same(my_lin: MyLinear, torch_lin: torch.nn.Linear):\n",
        "  \"\"\"In place modifies torch_lin to have same wts as my_lin\n",
        "  \"\"\"\n",
        "  state_dict =  {\n",
        "        \"weight\": torch.tensor(my_lin.weight.transpose()),\n",
        "  }\n",
        "  if my_lin.has_bias:\n",
        "    state_dict[\"bias\"]: torch.tensor(my_lin.bias)\n",
        "  torch_lin.load_state_dict(state_dict)\n",
        "\n",
        "def make_similar_linear(my_lin: MyLinear) -> torch.nn.Linear:\n",
        "  \"\"\"Returns a torch.nn.Linear class that has the same weights as MyLinear\n",
        "  \"\"\"\n",
        "  rv = torch.nn.Linear(in_features = my_lin.in_features, out_features=my_lin.out_features, bias=my_lin.has_bias, dtype=torch.float32)\n",
        "  make_wts_same(my_lin, rv)\n",
        "  return rv\n",
        "\n",
        "\n",
        "class MyReLU:\n",
        "  def __init__():\n",
        "    self.stored_inp_activations: List[np.array] = []\n",
        "\n",
        "  def forward(self, activations: np.array) -> np.array:\n",
        "    self.stored_inp_activations.append(activations)\n",
        "    return np.where(activations>0, activations, 0)\n",
        "\n",
        "  def backward(self, output_act_grad: np.array) -> np.array:\n",
        "    # output_act_grad: (bsz, d_model)\n",
        "    # self.stored_inp_activations: (bsz, d_model)\n",
        "    inp_activations = np.concat(self.stored_inp_activations)\n",
        "    input_act_grad = np.where(inp_activations > 0, inp_activations, 0)\n",
        "    return input_act_grad.sum(axis=0)\n",
        "\n",
        "\n",
        "def make_two_linears(input_dim, output_dim):\n",
        "  custom_lin =  MyLinear(input_dim, output_dim, False)\n",
        "  torch_lin = make_similar_linear(custom_lin)\n",
        "  return custom_lin, torch_lin\n",
        "\n",
        "def clear_grads(custom_lin, torch_lin):\n",
        "  for p in torch_lin.parameters():\n",
        "    if p.grad is not None:\n",
        "      p.grad = None\n",
        "  custom_lin.clear_grads()\n",
        "\n",
        "\n",
        "def get_random_act(in_features, bsz):\n",
        "    # create random activations in numpy\n",
        "    random_act = np.random.normal(0, 1, size=(bsz, in_features)).astype(np.float32)\n",
        "    # convert to torch tensor\n",
        "    torch_random_act = torch.tensor(random_act, dtype=torch.float32, requires_grad=True)\n",
        "    return random_act, torch_random_act\n",
        "\n",
        "\n",
        "def compare_linear(my_lin, torch_lin, in_features):\n",
        "    random_act, torch_random_act = get_random_act(in_features, BSZ)\n",
        "    # forward passes\n",
        "    my_act = my_lin.forward(random_act)\n",
        "    torch_act = torch_lin(torch_random_act)\n",
        "\n",
        "    # check they match (within tolerance)\n",
        "    assert np.all(np.isclose(my_act, torch_act.detach().numpy()))\n",
        "    return my_act, torch_act, random_act, torch_random_act\n",
        ""
      ],
      "metadata": {
        "id": "JJR4wyfayeL-"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Match Forward Backward one layer"
      ],
      "metadata": {
        "id": "NfqjPkU_m53Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def match_fwd_bwd_one_mlp_simple_loss():\n",
        "  my_linear1, torch_linear1 = make_two_linears(D_MODEL, D_FF)\n",
        "  my_output_act, torch_output_act, my_input_act, torch_input_act = compare_linear(my_linear1, torch_linear1, D_MODEL)\n",
        "  print(\"fwd matches\")\n",
        "\n",
        "  torch_loss = torch_output_act.sum()\n",
        "  torch_loss.backward()\n",
        "\n",
        "  my_loss = my_output_act.sum() #dloss/d_my_act = 1\n",
        "  my_inp_act_grad = my_linear1.backward(output_act_grad=np.ones_like(my_output_act))\n",
        "\n",
        "  assert np.all(np.allclose(my_linear1.weight_grad, torch_linear1.weight.grad.t()))\n",
        "  assert np.all(np.allclose(my_inp_act_grad, torch_input_act.grad))\n",
        "  print(\"Bwd matches\")\n",
        "\n",
        "\n",
        "def match_fwd_bwd_one_mlp_complex_loss():\n",
        "  my_linear1, torch_linear1 = make_two_linears(D_MODEL, D_FF)\n",
        "  my_output_act, torch_output_act, my_input_act, torch_input_act = compare_linear(my_linear1, torch_linear1, D_MODEL)\n",
        "  print(\"fwd matches\")\n",
        "\n",
        "  torch_loss = torch_output_act.square().sum()/2\n",
        "  torch_loss.backward()\n",
        "\n",
        "  my_loss = np.square(my_output_act).sum()/2  # grad = my_output_act\n",
        "  my_inp_act_grad = my_linear1.backward(output_act_grad=my_output_act)\n",
        "\n",
        "  assert np.all(np.allclose(my_linear1.weight_grad, torch_linear1.weight.grad.t()))\n",
        "  assert np.all(np.allclose(my_inp_act_grad, torch_input_act.grad))\n",
        "  print(\"Bwd matches\")\n"
      ],
      "metadata": {
        "id": "3eJTqWiS05Vf"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Match grad with simple dl/dy"
      ],
      "metadata": {
        "id": "57P-vUGK3u2W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "match_fwd_bwd_one_mlp_simple_loss()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "50F_m4t0yyIW",
        "outputId": "4c025fa8-2e43-40b1-edcf-b1bca3a51df4"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MyLinear.forward(), batch=1\n",
            "fwd matches\n",
            "MyLinear.bwd(), batches:1, act.shape:(2, 3)\n",
            "Bwd matches\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# more complex grad"
      ],
      "metadata": {
        "id": "-bVp2O4c7HN7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "match_fwd_bwd_one_mlp_complex_loss()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_EUGcfpw2E0r",
        "outputId": "d9458f25-0e99-4bf5-d0a9-0cd95c1f3cd4"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MyLinear.forward(), batch=1\n",
            "fwd matches\n",
            "MyLinear.bwd(), batches:1, act.shape:(2, 3)\n",
            "Bwd matches\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# MLP"
      ],
      "metadata": {
        "id": "qFlAgI5L6_PS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# linear --> ReLu --> linear\n",
        "\n",
        "class TorchMLP(torch.nn.Module):\n",
        "  def __init__(self, d_model, d_ffn):\n",
        "    super().__init__()\n",
        "    self.linear1 = torch.nn.Linear(in_features=d_model, out_features=d_ffn)\n",
        "    self.linear2 = torch.nn.Linear(in_features=d_ffn, out_features=d_model)\n",
        "\n",
        "  def forward(self, inp_act: torch.Tensor):\n",
        "    linear1_out = self.linear1(inp_act)\n",
        "    relu_out = torch.nn.functional.relu(linear1_out)\n",
        "    return self.linear2(relu_out)\n",
        "\n",
        "\n",
        "class MyMLP:\n",
        "  def __init__(self, d_model, d_ffn):\n",
        "    self.linear1 = MyLinear(in_features=d_model, out_features=d_ffn)\n",
        "    self.relu = MyReLU()\n",
        "    self.linear2 = MyLinear(in_features=d_ffn, out_features=d_model)\n",
        "\n",
        "  def forward(self, inp_act: torch.Tensor):\n",
        "    linear1_out = self.linear1.forward(inp_act)\n",
        "    relu_out = self.relu.forward(linear1_out)\n",
        "    linear2_out = self.linear2.forward(relu_out)\n",
        "    return linear2_out\n",
        "\n",
        "  def backward(self, output_act_grad: torch.Tensor):\n",
        "    linear2_input_act_grad = self.linear2.backward(output_act_grad)\n",
        "    relu_input_act_grad = self.relu.backward(linear2_input_act_grad)\n",
        "    linear1_input_act_grad = self.linear1.backward(relu_input_act_grad)\n",
        "\n",
        "\n",
        "  def clear_grads(self):\n",
        "    self.linear1.clear_grads()\n",
        "    self.relu.clear_grads()\n",
        "    self.linear2.clear_grads()\n",
        "\n",
        "\n",
        "def make_mlp_wts_same(my_mlp: MyMLP, torch_mlp: TorchMLP):\n",
        "  make_wts_same(my_mlp.linear1, torch_mlp.linear1)\n",
        "  make_wts_same(my_mlp.linear2, torch_mlp.linear2)\n"
      ],
      "metadata": {
        "id": "8L9pfXNa5GNi"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "custom_mlp = MyMLP(D_MODEL, D_FF)\n",
        "torch_mlp = TorchMLP(D_MODEL, D_FF)\n",
        "\n"
      ],
      "metadata": {
        "id": "hvpirOJK6VF7"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}