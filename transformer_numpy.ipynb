{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNwqXbM+Ix5cboz3CM2Zn1I",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kmalik22/colabs/blob/main/transformer_numpy.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "id": "IxBz_g1wyZ-e"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "from typing import List"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "D_MODEL = 3\n",
        "D_FF = int(D_MODEL * 2)\n",
        "BSZ = 2"
      ],
      "metadata": {
        "id": "T_yJ3Obf29bU"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torchlin = torch.nn.Linear(2, 4, True)\n",
        "torchlin.weight.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7qqIwpi30Vun",
        "outputId": "37c6925e-eb38-4104-df38-d63808258ea4"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([4, 2])"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#class torch.nn.Linear(in_features, out_features, bias=True, device=None, dtype=None)\n",
        "class MyLinear:\n",
        "  def __init__(self, in_features, out_features, bias=True, debug=True):\n",
        "    # create a matrix (in_features, out_features)\n",
        "    # backprop. During forward, store activations in a buffer.\n",
        "    self.in_features = in_features\n",
        "    self.out_features = out_features\n",
        "    self.has_bias = bias\n",
        "    self.weight = np.random.normal(0, 1, size=(in_features, out_features))\n",
        "    self.bias = np.zeros(shape=(out_features))\n",
        "    self.stored_activations: List[np.array] = []\n",
        "    self.weight_grad = None\n",
        "    self.debug = debug\n",
        "\n",
        "  def forward(self, activations: np.array):\n",
        "    assert len(activations.shape) >= 2\n",
        "    assert activations.shape[-1] == self.in_features\n",
        "    self.stored_activations.append(activations)\n",
        "    if self.debug:\n",
        "       print(f\"MyLinear.forward(), batch={len(self.stored_activations)}\")\n",
        "    return ( (activations @ self.weight) + self.bias)\n",
        "\n",
        "  def backward(self, output_act_grad: np.array) -> np.array:\n",
        "    \"\"\"Computes weight grad internally. Returns input_act_grad\n",
        "    \"\"\"\n",
        "    assert len(self.stored_activations > 0)\n",
        "    input_acts = np.concat(self.stored_activations)\n",
        "    if self.debug:\n",
        "      print(f\"MyLinear.bwd(), batches:{len(self.stored_activations)}, act.shape:{input_acts.shape}\")\n",
        "    # input_acts = (bsz in_features)\n",
        "    # output_act_grad = (bsz out_features)\n",
        "    # wts = (in_features out_features)\n",
        "    # wgrad = (in_features out_features)\n",
        "    bsz = input_acts.shape[0]\n",
        "    assert input_acts.shape[1] == self.in_features, f\"{input_acts.shape[1]=} {self.in_features=}\"\n",
        "    assert output_act_grad.shape == (bsz, self.out_features), f\"{output_act_grad.shape=} {bsz}\"\n",
        "    wgrad = input_acts.transpose() @ output_act_grad\n",
        "\n",
        "  def clear_grad_state(self):\n",
        "    self.stored_activations = []\n",
        "    self.weight_grad = None\n",
        "\n",
        "\n",
        "def make_similar_linear(my_lin: MyLinear) -> torch.nn.Linear:\n",
        "  \"\"\"Returns a torch.nn.Linear class that has the same weights as MyLinear\n",
        "  \"\"\"\n",
        "  rv = torch.nn.Linear(in_features = my_lin.in_features, out_features=my_lin.out_features, bias=my_lin.has_bias, dtype=torch.float32)\n",
        "  # torch stores weight as (out_features, in_features), so need to transpose\n",
        "  rv.load_state_dict(\n",
        "      {\n",
        "        \"weight\": torch.tensor(my_lin.weight.transpose()),\n",
        "        \"bias\": torch.tensor(my_lin.bias),\n",
        "      }\n",
        "  )\n",
        "  return rv\n"
      ],
      "metadata": {
        "id": "JJR4wyfayeL-"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "my_linear = MyLinear(D_MODEL, D_FF, True)\n",
        "torch_linear = make_similar_linear(my_linear)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "dGj4qn5I02Qp"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Match Forward"
      ],
      "metadata": {
        "id": "NfqjPkU_m53Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "random_act = np.random.normal(0, 1, size=(BSZ, D_MODEL)).astype(np.float32)\n",
        "\n",
        "my_act = my_linear.forward(random_act)\n",
        "torch_act = torch_linear(torch.tensor(random_act, dtype=torch.float32))\n",
        "assert np.all(np.isclose(my_act, torch_act.detach().numpy()))"
      ],
      "metadata": {
        "id": "3eJTqWiS05Vf"
      },
      "execution_count": 12,
      "outputs": []
    }
  ]
}