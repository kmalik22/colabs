{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOJyRJQ9onF7a2juJk3uPSj",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kmalik22/colabs/blob/main/transformer_numpy.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 158,
      "metadata": {
        "id": "IxBz_g1wyZ-e"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "from typing import List, Dict"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "D_MODEL = 6\n",
        "D_FF = int(D_MODEL * 2)\n",
        "BSZ = 3\n",
        "NUM_HEADS=2\n",
        "HEAD_DIM=3"
      ],
      "metadata": {
        "id": "T_yJ3Obf29bU"
      },
      "execution_count": 114,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "a = torch.randn(4,3*2*3)\n",
        "print(a)\n",
        "print(a.shape)\n",
        "a = a.view(-1,3,2,3)\n",
        "print(a)\n",
        "print(a.shape)\n",
        "a_q = a[:,0,:,].unsqueeze(1)\n",
        "print(a_q)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eAcEca9Kyzxa",
        "outputId": "6fe92d7b-0073-40b6-bd66-56c7c19b6786"
      },
      "execution_count": 112,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[-0.6523,  1.0613, -1.2650, -1.2302, -1.1413, -1.6999, -0.7850, -0.7248,\n",
            "          1.2839, -1.0496, -0.5912, -0.7294,  0.8382, -0.8667,  0.8024,  1.0884,\n",
            "         -2.0617,  2.0572],\n",
            "        [ 0.7778,  0.7235,  0.0120, -1.0593,  1.7993,  0.6040,  0.3087, -0.3143,\n",
            "         -1.2033,  0.5609,  1.2270,  0.8578,  0.4705,  0.8915,  0.6006, -0.0034,\n",
            "          1.4793, -2.0744],\n",
            "        [-0.6778,  1.7703,  1.8224,  0.5625,  0.7274,  1.2462, -1.3848,  0.7357,\n",
            "         -0.1815,  0.8841,  0.3919, -0.4501, -0.7336,  1.3862, -0.3414,  1.3715,\n",
            "         -0.1641,  1.7837],\n",
            "        [-1.0479, -1.1928, -2.0494,  0.3420, -0.2737, -0.9612,  0.2094,  1.7915,\n",
            "          0.3433, -0.7865,  0.6469, -0.0918,  1.1199, -1.4083,  0.7308,  0.5011,\n",
            "         -0.0671,  0.0912]])\n",
            "torch.Size([4, 18])\n",
            "tensor([[[[-0.6523,  1.0613, -1.2650],\n",
            "          [-1.2302, -1.1413, -1.6999]],\n",
            "\n",
            "         [[-0.7850, -0.7248,  1.2839],\n",
            "          [-1.0496, -0.5912, -0.7294]],\n",
            "\n",
            "         [[ 0.8382, -0.8667,  0.8024],\n",
            "          [ 1.0884, -2.0617,  2.0572]]],\n",
            "\n",
            "\n",
            "        [[[ 0.7778,  0.7235,  0.0120],\n",
            "          [-1.0593,  1.7993,  0.6040]],\n",
            "\n",
            "         [[ 0.3087, -0.3143, -1.2033],\n",
            "          [ 0.5609,  1.2270,  0.8578]],\n",
            "\n",
            "         [[ 0.4705,  0.8915,  0.6006],\n",
            "          [-0.0034,  1.4793, -2.0744]]],\n",
            "\n",
            "\n",
            "        [[[-0.6778,  1.7703,  1.8224],\n",
            "          [ 0.5625,  0.7274,  1.2462]],\n",
            "\n",
            "         [[-1.3848,  0.7357, -0.1815],\n",
            "          [ 0.8841,  0.3919, -0.4501]],\n",
            "\n",
            "         [[-0.7336,  1.3862, -0.3414],\n",
            "          [ 1.3715, -0.1641,  1.7837]]],\n",
            "\n",
            "\n",
            "        [[[-1.0479, -1.1928, -2.0494],\n",
            "          [ 0.3420, -0.2737, -0.9612]],\n",
            "\n",
            "         [[ 0.2094,  1.7915,  0.3433],\n",
            "          [-0.7865,  0.6469, -0.0918]],\n",
            "\n",
            "         [[ 1.1199, -1.4083,  0.7308],\n",
            "          [ 0.5011, -0.0671,  0.0912]]]])\n",
            "torch.Size([4, 3, 2, 3])\n",
            "tensor([[[[-0.6523,  1.0613, -1.2650],\n",
            "          [-1.2302, -1.1413, -1.6999]]],\n",
            "\n",
            "\n",
            "        [[[ 0.7778,  0.7235,  0.0120],\n",
            "          [-1.0593,  1.7993,  0.6040]]],\n",
            "\n",
            "\n",
            "        [[[-0.6778,  1.7703,  1.8224],\n",
            "          [ 0.5625,  0.7274,  1.2462]]],\n",
            "\n",
            "\n",
            "        [[[-1.0479, -1.1928, -2.0494],\n",
            "          [ 0.3420, -0.2737, -0.9612]]]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "uVB4fhXoy6uD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#class torch.nn.Linear(in_features, out_features, bias=True, device=None, dtype=None)\n",
        "class MyLinear:\n",
        "  def __init__(self, in_features, out_features, bias=True, debug=True):\n",
        "    # create a matrix (in_features, out_features)\n",
        "    # backprop. During forward, store activations in a buffer.\n",
        "    self.in_features = in_features\n",
        "    self.out_features = out_features\n",
        "    self.has_bias = bias\n",
        "    self.weight = np.random.normal(0, 1, size=(in_features, out_features))\n",
        "    self.bias = np.zeros(shape=(out_features))\n",
        "    self.stored_activations: List[np.array] = []\n",
        "    self.weight_grad = None\n",
        "    self.bias_grad = None\n",
        "    self.debug = debug\n",
        "\n",
        "  def forward(self, activations: np.array):\n",
        "    assert len(activations.shape) >= 2\n",
        "    assert activations.shape[-1] == self.in_features\n",
        "    self.stored_activations.append(activations)\n",
        "    if self.debug:\n",
        "       print(f\"MyLinear.forward(), batch={len(self.stored_activations)}\")\n",
        "    return ( (activations @ self.weight) + self.bias)\n",
        "\n",
        "  def backward(self, output_act_grad: np.array) -> np.array:\n",
        "    \"\"\"Computes weight grad internally. Returns input_act_grad\n",
        "    \"\"\"\n",
        "    assert len(self.stored_activations) > 0\n",
        "    input_acts = np.concat(self.stored_activations)\n",
        "    if self.debug:\n",
        "      print(f\"MyLinear.bwd(), batches:{len(self.stored_activations)}, act.shape:{input_acts.shape}\")\n",
        "    # input_acts = (bsz in_features)\n",
        "    # output_act_grad = (bsz out_features)\n",
        "    # wts = (in_features out_features)\n",
        "    # wgrad = (in_features out_features)\n",
        "    # input_act_grad = (bsz in_features)\n",
        "    bsz = input_acts.shape[0]\n",
        "    assert input_acts.shape[1] == self.in_features, f\"{input_acts.shape[1]=} {self.in_features=}\"\n",
        "    assert output_act_grad.shape == (bsz, self.out_features), f\"{output_act_grad.shape=} {bsz=} {self.out_features=}\"\n",
        "    self.weight_grad = input_acts.transpose() @ output_act_grad # (in_features, bsz) @ (bsz, out_features)\n",
        "    self.bias_grad = output_act_grad.sum(axis=0) # (out_features)\n",
        "    return output_act_grad @ self.weight.transpose()\n",
        "\n",
        "  def clear_grads(self):\n",
        "    self.stored_activations = []\n",
        "    self.weight_grad = None\n",
        "    self.bias_grad = None\n",
        "\n",
        "\n",
        "def make_wts_same(my_lin: MyLinear, torch_lin: torch.nn.Linear):\n",
        "  \"\"\"In place modifies torch_lin to have same wts as my_lin\n",
        "  \"\"\"\n",
        "  state_dict =  {\n",
        "        \"weight\": torch.tensor(my_lin.weight.transpose()),\n",
        "  }\n",
        "  if my_lin.has_bias:\n",
        "    state_dict[\"bias\"]: torch.tensor(my_lin.bias)\n",
        "  torch_lin.load_state_dict(state_dict)\n",
        "\n",
        "def make_similar_linear(my_lin: MyLinear) -> torch.nn.Linear:\n",
        "  \"\"\"Returns a torch.nn.Linear class that has the same weights as MyLinear\n",
        "  \"\"\"\n",
        "  rv = torch.nn.Linear(in_features = my_lin.in_features, out_features=my_lin.out_features, bias=my_lin.has_bias, dtype=torch.float32)\n",
        "  make_wts_same(my_lin, rv)\n",
        "  return rv\n",
        "\n",
        "\n",
        "class MyReLU:\n",
        "  def __init__(self):\n",
        "    self.stored_inp_activations: List[np.array] = []\n",
        "\n",
        "  def forward(self, activations: np.array) -> np.array:\n",
        "    self.stored_inp_activations.append(activations)\n",
        "    return np.where(activations>0, activations, 0)\n",
        "\n",
        "  def backward(self, output_act_grad: np.array) -> np.array:\n",
        "    # output_act_grad: (bsz, d_model)\n",
        "    # self.stored_inp_activations: (bsz, d_model)\n",
        "    inp_activations = np.concat(self.stored_inp_activations)\n",
        "    input_act_grad = np.where(inp_activations >= 0, output_act_grad, 0)\n",
        "    return input_act_grad\n",
        "\n",
        "\n",
        "def make_two_linears(input_dim, output_dim):\n",
        "  custom_lin =  MyLinear(input_dim, output_dim, False)\n",
        "  torch_lin = make_similar_linear(custom_lin)\n",
        "  return custom_lin, torch_lin\n",
        "\n",
        "def clear_grads(custom_lin, torch_lin):\n",
        "  for p in torch_lin.parameters():\n",
        "    if p.grad is not None:\n",
        "      p.grad = None\n",
        "  custom_lin.clear_grads()\n",
        "\n",
        "\n",
        "def get_random_act(in_features, bsz):\n",
        "    # create random activations in numpy\n",
        "    random_act = np.random.normal(0, 1, size=(bsz, in_features)).astype(np.float32)\n",
        "    # convert to torch tensor\n",
        "    torch_random_act = torch.tensor(random_act, dtype=torch.float32, requires_grad=True)\n",
        "    return random_act, torch_random_act\n",
        "\n",
        "\n",
        "def compare_forward(my_lin, torch_lin, in_features):\n",
        "    random_act, torch_random_act = get_random_act(in_features, BSZ)\n",
        "    # forward passes\n",
        "    my_act = my_lin.forward(random_act)\n",
        "    torch_act = torch_lin(torch_random_act)\n",
        "\n",
        "    # check they match (within tolerance)\n",
        "    assert np.all(np.isclose(my_act, torch_act.detach().numpy()))\n",
        "    print(\"fwd matches\")\n",
        "    return my_act, torch_act, random_act, torch_random_act\n",
        ""
      ],
      "metadata": {
        "id": "JJR4wyfayeL-"
      },
      "execution_count": 94,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Match Forward Backward one layer"
      ],
      "metadata": {
        "id": "NfqjPkU_m53Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def match_fwd_bwd_one_mlp_simple_loss():\n",
        "  my_linear1, torch_linear1 = make_two_linears(D_MODEL, D_FF)\n",
        "  my_output_act, torch_output_act, my_input_act, torch_input_act = compare_forward(my_linear1, torch_linear1, D_MODEL)\n",
        "\n",
        "  torch_loss = torch_output_act.sum()\n",
        "  torch_loss.backward()\n",
        "\n",
        "  my_loss = my_output_act.sum() #dloss/d_my_act = 1\n",
        "  my_inp_act_grad = my_linear1.backward(output_act_grad=np.ones_like(my_output_act))\n",
        "\n",
        "  assert np.all(np.allclose(my_linear1.weight_grad, torch_linear1.weight.grad.t()))\n",
        "  assert np.all(np.allclose(my_inp_act_grad, torch_input_act.grad))\n",
        "  print(\"Bwd matches\")\n",
        "\n",
        "\n",
        "def match_fwd_bwd_one_mlp_complex_loss():\n",
        "  my_linear1, torch_linear1 = make_two_linears(D_MODEL, D_FF)\n",
        "  my_output_act, torch_output_act, my_input_act, torch_input_act = compare_forward(my_linear1, torch_linear1, D_MODEL)\n",
        "  print(\"fwd matches\")\n",
        "\n",
        "  torch_loss = torch_output_act.square().sum()/2\n",
        "  torch_loss.backward()\n",
        "\n",
        "  my_loss = np.square(my_output_act).sum()/2  # grad = my_output_act\n",
        "  my_inp_act_grad = my_linear1.backward(output_act_grad=my_output_act)\n",
        "\n",
        "  assert np.all(np.allclose(my_linear1.weight_grad, torch_linear1.weight.grad.t()))\n",
        "  assert np.all(np.allclose(my_inp_act_grad, torch_input_act.grad))\n",
        "  print(\"Bwd matches\")\n"
      ],
      "metadata": {
        "id": "3eJTqWiS05Vf"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Match grad with simple dl/dy"
      ],
      "metadata": {
        "id": "57P-vUGK3u2W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "match_fwd_bwd_one_mlp_simple_loss()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "50F_m4t0yyIW",
        "outputId": "ebb08bb1-c2d4-401f-9480-9fdbca556e98"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MyLinear.forward(), batch=1\n",
            "fwd matches\n",
            "MyLinear.bwd(), batches:1, act.shape:(2, 3)\n",
            "Bwd matches\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# more complex grad"
      ],
      "metadata": {
        "id": "-bVp2O4c7HN7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "match_fwd_bwd_one_mlp_complex_loss()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_EUGcfpw2E0r",
        "outputId": "63932823-16e3-4a18-85dc-7e23e69c42a6"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MyLinear.forward(), batch=1\n",
            "fwd matches\n",
            "MyLinear.bwd(), batches:1, act.shape:(2, 3)\n",
            "Bwd matches\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# MLP"
      ],
      "metadata": {
        "id": "qFlAgI5L6_PS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# linear --> ReLu --> linear\n",
        "\n",
        "class TorchMLP(torch.nn.Module):\n",
        "  def __init__(self, d_model, d_ffn):\n",
        "    super().__init__()\n",
        "    self.linear1 = torch.nn.Linear(in_features=d_model, out_features=d_ffn, bias=False)\n",
        "    self.linear2 = torch.nn.Linear(in_features=d_ffn, out_features=d_model, bias=False)\n",
        "\n",
        "  def forward(self, inp_act: torch.Tensor):\n",
        "    self.linear1_out = self.linear1(inp_act)\n",
        "    self.linear1_out.retain_grad()\n",
        "    self.relu_out = torch.nn.functional.relu(self.linear1_out)\n",
        "    self.relu_out.retain_grad()\n",
        "    self.linear2_out = self.linear2(self.relu_out)\n",
        "    self.linear2_out.retain_grad()\n",
        "    return self.linear2_out\n",
        "\n",
        "\n",
        "class MyMLP:\n",
        "  def __init__(self, d_model, d_ffn):\n",
        "    self.linear1 = MyLinear(in_features=d_model, out_features=d_ffn)\n",
        "    self.relu = MyReLU()\n",
        "    self.linear2 = MyLinear(in_features=d_ffn, out_features=d_model)\n",
        "\n",
        "  def forward(self, inp_act: torch.Tensor):\n",
        "    linear1_out = self.linear1.forward(inp_act)\n",
        "    relu_out = self.relu.forward(linear1_out)\n",
        "    linear2_out = self.linear2.forward(relu_out)\n",
        "    return linear2_out\n",
        "\n",
        "  def backward(self, output_act_grad: torch.Tensor):\n",
        "    self.linear2_input_act_grad = self.linear2.backward(output_act_grad)\n",
        "    self.relu_input_act_grad = self.relu.backward(self.linear2_input_act_grad)\n",
        "    self.linear1_input_act_grad = self.linear1.backward(self.relu_input_act_grad)\n",
        "    return self.linear1_input_act_grad\n",
        "\n",
        "\n",
        "  def clear_grads(self):\n",
        "    self.linear1.clear_grads()\n",
        "    self.relu.clear_grads()\n",
        "    self.linear2.clear_grads()\n",
        "\n",
        "\n",
        "def make_mlp_wts_same(my_mlp: MyMLP, torch_mlp: TorchMLP):\n",
        "  make_wts_same(my_mlp.linear1, torch_mlp.linear1)\n",
        "  make_wts_same(my_mlp.linear2, torch_mlp.linear2)\n",
        "\n",
        "\n",
        "def match_fwd_bwd_full_mlp():\n",
        "  custom_mlp = MyMLP(D_MODEL, D_FF)\n",
        "  torch_mlp = TorchMLP(D_MODEL, D_FF)\n",
        "  make_mlp_wts_same(custom_mlp, torch_mlp)\n",
        "  my_output_act, torch_output_act, my_input_act, torch_input_act = compare_forward(custom_mlp, torch_mlp, D_MODEL)\n",
        "\n",
        "  torch_loss = torch_output_act.square().sum()/2\n",
        "  torch_loss.backward()\n",
        "\n",
        "  my_loss = np.square(my_output_act).sum()/2  # grad = my_output_act\n",
        "  my_inp_act_grad = custom_mlp.backward(output_act_grad=my_output_act)\n",
        "\n",
        "\n",
        "  assert np.all(np.allclose(custom_mlp.linear2.weight_grad, torch_mlp.linear2.weight.grad.t()))\n",
        "  assert np.all(np.allclose(custom_mlp.linear1.weight_grad, torch_mlp.linear1.weight.grad.t()))\n",
        "  print(f\"bwd matches\")\n",
        "\n",
        "  def debug_grads():\n",
        "    assert np.all(np.allclose(custom_mlp.linear2.weight_grad, torch_mlp.linear2.weight.grad.t()))\n",
        "    print(f\"Linear2 weight grad matches\")\n",
        "\n",
        "    assert np.all(np.allclose(custom_mlp.linear2_input_act_grad, torch_mlp.relu_out.grad))\n",
        "    print(f\"Linear2 input act grad matches\")\n",
        "\n",
        "    assert np.all(np.allclose(custom_mlp.relu_input_act_grad, torch_mlp.linear1_out.grad)), f\"{custom_mlp.relu_input_act_grad}\\n{torch_mlp.linear1_out.grad}\"\n",
        "    print(f\"Relu input act grad matches\")\n",
        "\n",
        "    assert np.all(np.allclose(custom_mlp.relu_input_act_grad, torch_mlp.linear1_out.grad)), f\"{custom_mlp.relu_input_act_grad}\\n{torch_mlp.linear1_out.grad}\"\n",
        "    print(f\"Relu input act grad matches\")\n"
      ],
      "metadata": {
        "id": "8L9pfXNa5GNi"
      },
      "execution_count": 101,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "match_fwd_bwd_full_mlp()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hvpirOJK6VF7",
        "outputId": "49f9bb7c-9319-4cb6-a827-1080f43eb8d7"
      },
      "execution_count": 102,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MyLinear.forward(), batch=1\n",
            "MyLinear.forward(), batch=1\n",
            "fwd matches\n",
            "MyLinear.bwd(), batches:1, act.shape:(2, 6)\n",
            "MyLinear.bwd(), batches:1, act.shape:(2, 3)\n",
            "bwd matches\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Attention"
      ],
      "metadata": {
        "id": "Wdzp0nm9xClF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class TorchMultiheadAttention(torch.nn.Module):\n",
        "  def __init__(self, num_heads, head_dim):\n",
        "    super().__init__()\n",
        "    self.d_model = int(num_heads*head_dim)\n",
        "    self.num_heads = num_heads\n",
        "    self.head_dim = head_dim\n",
        "    self.wqkv = torch.nn.Linear(self.d_model, int(3*num_heads*head_dim), bias=False)\n",
        "    self.wo = torch.nn.Linear(self.d_model, self.d_model, bias=False)\n",
        "\n",
        "  def softmax(self, wts):\n",
        "    #wts.shape = num_heads,seqlen,seqlen\n",
        "    #returns: (num_heads, seqlen, seqlen)\n",
        "    #1. compute max across columns\n",
        "    #2. subtract max\n",
        "    #3. numerator = exponentiate\n",
        "    #4. denominator = sum of exponentiated across columns\n",
        "    #5. return num/denom\n",
        "    rowwise_max = wts.amax(dim=2).unsqueeze(-1)  #(num_heads,seqlen,1)\n",
        "    max_subtracted = wts - rowwise_max  #(num_heads, seqlen, seqlen) TODO check broadcasting\n",
        "    numerator = max_subtracted.exp() #(num_heads, seqlen, seqlen)\n",
        "    denominator = numerator.sum(dim=2).unsqueeze(-1) #(num_heads, seqlen, 1)\n",
        "    return numerator/denominator\n",
        "\n",
        "  def scaled_dot_product_attn(self, q, k, v):\n",
        "    # q,k,v: num_heads,seqlen,head_dim\n",
        "    head_dim=q.shape[-1]\n",
        "    seqlen=q.shape[-2]\n",
        "    #q = (num_heads,seqlen,head_dim)\n",
        "    #k.t() = (num_head, head_dim, seqlen)\n",
        "    attn_wts = (q @ k.transpose(1,2))/(head_dim**0.5) # (num_heads, seqlen, seqlen)\n",
        "    mask = torch.tril(torch.ones((seqlen,seqlen))).bool()\n",
        "    attn_wts_masked = torch.where(mask, attn_wts, float('-inf'))\n",
        "    attn_wts_softmax = self.softmax(attn_wts_masked)\n",
        "    # attn_wts_softmax = (num_heads, seqlen, seqlen)\n",
        "    # v = (num_heads, seqlen, head_dim)\n",
        "    output = attn_wts_softmax @ v  # (num_heads, seqlen, head_dim)\n",
        "    return output\n",
        "\n",
        "  def forward(self, x: torch.Tensor):\n",
        "    seqlen = x.shape[0]\n",
        "    d_model = x.shape[1]\n",
        "    assert d_model == self.d_model\n",
        "    self.wqkv_out = self.wqkv(x) # x=(seqlen,d_model) wqkv=(d_model,3*num_heads*head_dim). out=(seqlen,3*num_heads*head_dim)\n",
        "    assert self.wqkv_out.shape == (seqlen, int(3*self.num_heads*self.head_dim))\n",
        "    self.wqkv_out = self.wqkv_out.view(-1,3,self.num_heads,self.head_dim)\n",
        "    assert self.wqkv_out.shape == (seqlen, 3, self.num_heads, self.head_dim)\n",
        "    self.wqkv_out.retain_grad()\n",
        "\n",
        "    self.q = self.wqkv_out[:,0,:,:].transpose(0,1) # num_heads,seqlen,head_dim\n",
        "    self.k = self.wqkv_out[:,1,:,:].transpose(0,1) # num_heads,seqlen,head_dim\n",
        "    self.v = self.wqkv_out[:,2,:,:].transpose(0,1) # num_heads,seqlen,head_dim\n",
        "    self.q.retain_grad()\n",
        "    self.k.retain_grad()\n",
        "    self.v.retain_grad()\n",
        "    assert self.q.shape == self.k.shape == self.v.shape == (self.num_heads,seqlen,self.head_dim), f\"{self.q.shape} {self.k.shape} {self.v.shape} {(self.num_heads, seqlen, self.head_dim)}\"\n",
        "\n",
        "    self.attn_out = self.scaled_dot_product_attn(self.q, self.k, self.v)\n",
        "    assert self.attn_out.shape == (self.num_heads,seqlen,self.head_dim)\n",
        "    self.attn_out_concat = self.attn_out.transpose(0,1).reshape(-1,self.d_model)  # (seqlen,d_model)\n",
        "    assert self.attn_out_concat.shape == (seqlen, self.d_model)\n",
        "    self.out = self.wo(self.attn_out_concat)\n",
        "    return self.out\n",
        "\n",
        "\n",
        "\n",
        "class MyMultiheadAttention:\n",
        "  def __init__(self, num_heads, head_dim):\n",
        "    self.num_heads = num_heads\n",
        "    self.head_dim = head_dim\n",
        "    self.d_model = int(num_heads*head_dim)\n",
        "    self.wqkv = MyLinear(self.d_model, 3*num_heads*head_dim, bias=False)\n",
        "    self.wo = MyLinear(self.d_model, self.d_model, bias=False)\n",
        "    self.saved_activations = Dict[str, List[torch.Tensor]] = {}\n",
        "\n",
        "\n",
        "  def forward(self, x):\n",
        "    self.wqkv_out = self.wqkv.forward(x) #wqkv_out = (seqlen, 3*num_heads*head_dim)\n",
        "    self.wqkv_out_alt =\n",
        "    self.q = self.wqkv_out[:,]\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Y8HiHJLzxDk9"
      },
      "execution_count": 156,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch_attn = TorchMultiheadAttention(num_heads=NUM_HEADS, head_dim=HEAD_DIM)\n",
        "random_act, torch_random_act = get_random_act(D_MODEL, BSZ)\n",
        "torch_attn_out = torch_attn(torch_random_act)"
      ],
      "metadata": {
        "id": "AGvZNuOz5YmP"
      },
      "execution_count": 157,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "rupcpGTc5kNU"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}