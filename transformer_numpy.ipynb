{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOw5l7KAtd3o0uzmhRFZtkI",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kmalik22/colabs/blob/main/transformer_numpy.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "id": "IxBz_g1wyZ-e"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "from typing import List"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "D_MODEL = 3\n",
        "D_FF = int(D_MODEL * 2)\n",
        "BSZ = 2"
      ],
      "metadata": {
        "id": "T_yJ3Obf29bU"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch_linear.weight.grad"
      ],
      "metadata": {
        "id": "7qqIwpi30Vun"
      },
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#class torch.nn.Linear(in_features, out_features, bias=True, device=None, dtype=None)\n",
        "class MyLinear:\n",
        "  def __init__(self, in_features, out_features, bias=True, debug=True):\n",
        "    # create a matrix (in_features, out_features)\n",
        "    # backprop. During forward, store activations in a buffer.\n",
        "    self.in_features = in_features\n",
        "    self.out_features = out_features\n",
        "    self.has_bias = bias\n",
        "    self.weight = np.random.normal(0, 1, size=(in_features, out_features))\n",
        "    self.bias = np.zeros(shape=(out_features))\n",
        "    self.stored_activations: List[np.array] = []\n",
        "    self.weight_grad = None\n",
        "    self.bias_grad = None\n",
        "    self.debug = debug\n",
        "\n",
        "  def forward(self, activations: np.array):\n",
        "    assert len(activations.shape) >= 2\n",
        "    assert activations.shape[-1] == self.in_features\n",
        "    self.stored_activations.append(activations)\n",
        "    if self.debug:\n",
        "       print(f\"MyLinear.forward(), batch={len(self.stored_activations)}\")\n",
        "    return ( (activations @ self.weight) + self.bias)\n",
        "\n",
        "  def backward(self, output_act_grad: np.array) -> np.array:\n",
        "    \"\"\"Computes weight grad internally. Returns input_act_grad\n",
        "    \"\"\"\n",
        "    assert len(self.stored_activations) > 0\n",
        "    input_acts = np.concat(self.stored_activations)\n",
        "    if self.debug:\n",
        "      print(f\"MyLinear.bwd(), batches:{len(self.stored_activations)}, act.shape:{input_acts.shape}\")\n",
        "    # input_acts = (bsz in_features)\n",
        "    # output_act_grad = (bsz out_features)\n",
        "    # wts = (in_features out_features)\n",
        "    # wgrad = (in_features out_features)\n",
        "    # input_act_grad = (bsz in_features)\n",
        "    bsz = input_acts.shape[0]\n",
        "    assert input_acts.shape[1] == self.in_features, f\"{input_acts.shape[1]=} {self.in_features=}\"\n",
        "    assert output_act_grad.shape == (bsz, self.out_features), f\"{output_act_grad.shape=} {bsz}\"\n",
        "    self.weight_grad = input_acts.transpose() @ output_act_grad # (in_features, bsz) @ (bsz, out_features)\n",
        "    self.bias_grad = output_act_grad.sum(axis=0) # (out_features)\n",
        "    return output_act_grad @ self.weight.transpose()\n",
        "\n",
        "  def clear_grad_state(self):\n",
        "    self.stored_activations = []\n",
        "    self.weight_grad = None\n",
        "    self.bias_grad = None\n",
        "\n",
        "\n",
        "def make_similar_linear(my_lin: MyLinear) -> torch.nn.Linear:\n",
        "  \"\"\"Returns a torch.nn.Linear class that has the same weights as MyLinear\n",
        "  \"\"\"\n",
        "  rv = torch.nn.Linear(in_features = my_lin.in_features, out_features=my_lin.out_features, bias=my_lin.has_bias, dtype=torch.float32)\n",
        "  # torch stores weight as (out_features, in_features), so need to transpose\n",
        "  state_dict =  {\n",
        "        \"weight\": torch.tensor(my_lin.weight.transpose()),\n",
        "  }\n",
        "  if my_lin.has_bias:\n",
        "    state_dict[\"bias\"]: torch.tensor(my_lin.bias)\n",
        "  rv.load_state_dict(state_dict)\n",
        "  return rv\n"
      ],
      "metadata": {
        "id": "JJR4wyfayeL-"
      },
      "execution_count": 67,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "my_linear = MyLinear(D_MODEL, D_FF, False)\n",
        "torch_linear = make_similar_linear(my_linear)\n",
        "\n",
        "def clear_grads():\n",
        "  for p in torch_linear.parameters():\n",
        "    if p.grad is not None:\n",
        "      p.grad = None\n",
        "\n",
        "  my_linear.clear_grad_state()"
      ],
      "metadata": {
        "id": "dGj4qn5I02Qp"
      },
      "execution_count": 138,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Match Forward"
      ],
      "metadata": {
        "id": "NfqjPkU_m53Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def compare_linear(my_linear, torch_linear):\n",
        "    # create random activations in numpy\n",
        "    random_act = np.random.normal(0, 1, size=(BSZ, D_MODEL)).astype(np.float32)\n",
        "    # convert to torch tensor\n",
        "    torch_random_act = torch.tensor(random_act, dtype=torch.float32, requires_grad=True)\n",
        "\n",
        "    # forward passes\n",
        "    my_act = my_linear.forward(random_act)\n",
        "    torch_act = torch_linear(torch_random_act)\n",
        "\n",
        "    # check they match (within tolerance)\n",
        "    assert np.all(np.isclose(my_act, torch_act.detach().numpy()))\n",
        "    return my_act, torch_act, random_act, torch_random_act\n",
        "\n",
        "my_output_act, torch_output_act, my_input_act, torch_input_act = compare_linear(my_linear, torch_linear)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3eJTqWiS05Vf",
        "outputId": "e399ad98-fc0f-4235-aaa8-b25c59aef4c9"
      },
      "execution_count": 139,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MyLinear.forward(), batch=1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Match grad with simple dl/dy"
      ],
      "metadata": {
        "id": "57P-vUGK3u2W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "torch_loss = torch_output_act.sum()\n",
        "torch_loss.backward()\n",
        "\n",
        "my_loss = my_output_act.sum() #dloss/d_my_act = 1\n",
        "my_inp_act_grad = my_linear.backward(output_act_grad=np.ones_like(my_act))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "50F_m4t0yyIW",
        "outputId": "e855c9ea-e7fb-46e3-fbd8-3ef9ef88a383"
      },
      "execution_count": 140,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MyLinear.bwd(), batches:1, act.shape:(2, 3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "assert np.all(np.allclose(my_linear.weight_grad, torch_linear.weight.grad.t()))\n",
        "assert np.all(np.allclose(my_inp_act_grad, torch_input_act.grad))"
      ],
      "metadata": {
        "id": "hCjuc9283x3I"
      },
      "execution_count": 141,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# more complex grad"
      ],
      "metadata": {
        "id": "-zsMRkF_6Uds"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "clear_grads()\n",
        "my_output_act, torch_output_act, my_input_act, torch_input_act = compare_linear(my_linear, torch_linear)\n",
        "torch_loss = torch_output_act.square().sum()/2\n",
        "torch_loss.backward()\n",
        "\n",
        "my_loss = np.square(my_output_act).sum()/2  # grad = my_output_act\n",
        "my_inp_act_grad = my_linear.backward(output_act_grad=my_output_act)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RT1AczkX2If-",
        "outputId": "aa0cb431-264c-4b82-fae6-31f76f73f7c8"
      },
      "execution_count": 149,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MyLinear.forward(), batch=1\n",
            "MyLinear.bwd(), batches:1, act.shape:(2, 3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "assert np.all(np.allclose(my_linear.weight_grad, torch_linear.weight.grad.t()))\n",
        "assert np.all(np.allclose(my_inp_act_grad, torch_input_act.grad))"
      ],
      "metadata": {
        "id": "pV9_oUHl6TrM"
      },
      "execution_count": 150,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "8L9pfXNa5GNi"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}