{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyO9RZMfwZwkG2I4YjZyfbyK",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kmalik22/colabs/blob/main/transformer_training_pytorch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 125,
      "metadata": {
        "id": "xKr0v3brEOJW"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import torch.nn.functional as F\n",
        "import itertools\n",
        "from typing import Optional\n",
        "from torch.utils.data import IterableDataset, DataLoader\n",
        "torch.set_printoptions(linewidth=250, precision=3)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Modules"
      ],
      "metadata": {
        "id": "FkrwnUY_aTuI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MLP(torch.nn.Module):\n",
        "  def __init__(self, d_model, d_ffn):\n",
        "    super().__init__()\n",
        "    self.d_model = d_model\n",
        "    self.d_ffn = d_ffn\n",
        "    self.fc1 = torch.nn.Linear(in_features=d_model, out_features=d_ffn, bias=False)\n",
        "    torch.nn.init.trunc_normal_(self.fc1.weight, mean=0, std=1/d_model**0.5)\n",
        "    self.fc2 = torch.nn.Linear(in_features=d_ffn, out_features=d_model, bias=False)\n",
        "    torch.nn.init.trunc_normal_(self.fc2.weight, mean=0, std=1/d_ffn**0.5, a=-3, b=3)\n",
        "\n",
        "  def forward(self, x: torch.Tensor):\n",
        "    fc1_out = self.fc1(x)\n",
        "    relu_out = torch.nn.functional.relu(fc1_out)\n",
        "    fc2_out = self.fc2(relu_out)\n",
        "    return fc2_out\n",
        "\n",
        "class MultiHeadAttention(torch.nn.Module):\n",
        "  def __init__(self, d_model, num_heads):\n",
        "    super().__init__()\n",
        "    self.d_model = d_model\n",
        "    self.num_heads = num_heads\n",
        "    self.head_dim = d_model//num_heads\n",
        "    self.wqkv = torch.nn.Linear(d_model, int(3*d_model), bias=False)\n",
        "    torch.nn.init.trunc_normal_(self.wqkv.weight, mean=0, std=1/d_model**0.5, a=-3, b=3)\n",
        "    self.wo = torch.nn.Linear(d_model, d_model)\n",
        "    torch.nn.init.trunc_normal_(self.wo.weight, mean=0, std=1/d_model**0.5, a=-3, b=3)\n",
        "    # TODO: add rope\n",
        "\n",
        "  def scaled_dot_product_attn(self, q, k, v):\n",
        "    # q, k, v = (bsz, num_heads, seqlen, head_dim)\n",
        "    # qk.t()/sqrt(head_dim)\n",
        "    # causal mask\n",
        "    # softmax\n",
        "    # @ v\n",
        "    seqlen = q.shape[2]\n",
        "    attn_wts = q @ k.transpose(2,3)  #(bsz,num_h,seqlen,head_dim) (bsz,num_h,head_dim,seqlen) -> (bsz,num_h,seqlen,seqlen)\n",
        "    # create mask, do torch.where. 1=use attn wts, else use -inf\n",
        "    mask = torch.tril(torch.ones(seqlen,seqlen)).to(torch.bool)\n",
        "    masked_attn_wts = torch.where(mask, attn_wts, float('-inf')) # (bsz,num_heads,seqlen,seqlen)\n",
        "\n",
        "    # softmax TODO check once\n",
        "    softmax_wts = torch.nn.functional.softmax(masked_attn_wts,dim=-1)\n",
        "    # (bsz,num_heads,seqlen,seqlen) @ (bsz,num_heasds,seqlen,head_dim) -> (bsz,num_heads,seqlen,head_dim)\n",
        "    return softmax_wts @ v\n",
        "\n",
        "\n",
        "  def forward(self, x:torch.Tensor):\n",
        "    #x = (bsz, seqlen, d_model)\n",
        "    bsz, seqlen, d_model = x.shape\n",
        "    assert d_model == self.d_model\n",
        "    wqkv_out = self.wqkv(x) # (bsz, seqlen, 3*d_model)\n",
        "    #print(f\"wqkv_out:{wqkv_out[0,0,:].norm()}\")\n",
        "    # rearrange so seqlen,head_dim are the last two dims\n",
        "    wqkv_out = wqkv_out.reshape(bsz, seqlen, 3, self.num_heads, self.head_dim)\n",
        "    #(bsz,seqlen,3,num_heads,head_dim) -> (bsz,3,num_heads,seqlen,head_dim)\n",
        "    #  0    1    2    3        4\n",
        "    wqkv_out = wqkv_out.permute(0, 2, 3, 1, 4)  # (bsz, 3, num_heads, seqlen, head_dim)\n",
        "    q = wqkv_out[:,0,:,:,:] # (bsz, num_heads, seqlen, head_dim)\n",
        "    k = wqkv_out[:,1,:,:,:] # (bsz, num_heads, seqlen, head_dim)\n",
        "    v = wqkv_out[:,2,:,:,:] # (bsz, num_heads, seqlen, head_dim)\n",
        "    # TODO: add rope to q and k\n",
        "    #print(f\"q:{q[0,:,0,:].norm()}, k:{k[0,:,0,:].norm()}, v:{v[0,:,0,:].norm()}\")\n",
        "    self_attn_out = self.scaled_dot_product_attn(q, k, v) #(bsz, num_heads, seqlen, head_dim)\n",
        "    #print(f\"self_attn_out:{self_attn_out[0,:,0,:].norm()}\")\n",
        "    # transpose\n",
        "    self_attn_out = self_attn_out.transpose(1,2) # (bsz,seqlen,num_heads,head_dim)\n",
        "    # concat the last dim\n",
        "    self_attn_out = self_attn_out.reshape(bsz,seqlen,-1)\n",
        "    # wo\n",
        "    self_attn_out = self_attn_out.reshape(bsz, seqlen, d_model)\n",
        "    wo_out = self.wo(self_attn_out)\n",
        "    return wo_out\n",
        "\n",
        "\n",
        "class TransformerBlock(torch.nn.Module):\n",
        "  def __init__(self, d_model, d_ffn, num_heads, max_seqlen):\n",
        "    super().__init__()\n",
        "    self.mlp_norm = torch.nn.RMSNorm(normalized_shape=d_model)\n",
        "    self.mlp = MLP(d_model=d_model, d_ffn=d_ffn)\n",
        "    self.attn_norm = torch.nn.RMSNorm(normalized_shape=d_model)\n",
        "    self.attn = MultiHeadAttention(d_model=d_model,num_heads=num_heads)\n",
        "\n",
        "\n",
        "  def forward(self, x:torch.Tensor):\n",
        "    # norm --> attn, add back to original --> norm --> mlp, add back\n",
        "    attn_norm_out = self.attn_norm(x)\n",
        "    #print(f\"attn_norm_out: {attn_norm_out[0,0,:]}\")\n",
        "    attn_out = self.attn(attn_norm_out)\n",
        "    #print(f\"attn_out: {attn_out[0,0,:]}\")\n",
        "    attn_out_post_resid = x + attn_out\n",
        "    mlp_norm_out = self.mlp_norm(attn_out_post_resid)\n",
        "    mlp_out = self.mlp(mlp_norm_out)\n",
        "    return mlp_out + attn_out_post_resid\n",
        "\n",
        "class Transformer(torch.nn.Module):\n",
        "  def __init__(self, d_model, d_ffn, vocab_size, num_layers, max_seqlen, num_heads, simple_pos_embed=False):\n",
        "    super().__init__()\n",
        "    self.max_seqlen = max_seqlen\n",
        "    self.embedding = torch.nn.Embedding(num_embeddings=vocab_size, embedding_dim=d_model)\n",
        "    torch.nn.init.trunc_normal_(self.embedding.weight, mean=0, std=1/d_model**0.5, a=-3, b=-3)\n",
        "\n",
        "    self.simple_pos_embed = simple_pos_embed\n",
        "    if simple_pos_embed:\n",
        "      self.pos_embedding = torch.nn.Embedding(num_embeddings=max_seqlen, embedding_dim=d_model)\n",
        "\n",
        "    self.output_layer = torch.nn.Linear(in_features=d_model, out_features=vocab_size)\n",
        "    torch.nn.init.trunc_normal_(self.output_layer.weight, mean=0, std=1/d_model**0.5, a=-3, b=3)\n",
        "    self.layers = torch.nn.ModuleList(\n",
        "        [TransformerBlock(d_model=d_model, d_ffn=d_ffn, num_heads=num_heads, max_seqlen=max_seqlen) for _ in range(num_layers)]\n",
        "    )\n",
        "    # TODO: do proper init\n",
        "    # add attention\n",
        "    # add layer norms\n",
        "\n",
        "  def forward(self, tokens: torch.Tensor):\n",
        "    # tokens: (bsz,seqlen,d_model)\n",
        "    seqlen = tokens.shape[1]\n",
        "    curr_out = self.embedding(tokens)\n",
        "    if self.simple_pos_embed:\n",
        "      curr_out = curr_out + self.pos_embedding(torch.arange(seqlen))\n",
        "    for l in self.layers:\n",
        "      curr_out = curr_out + l(curr_out)\n",
        "    logits = self.output_layer(curr_out)  # bsz,seqlen,vocab_size\n",
        "    return logits\n"
      ],
      "metadata": {
        "id": "KgGPaK6cERnA"
      },
      "execution_count": 271,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Test code"
      ],
      "metadata": {
        "id": "J3MNY6h3aQUh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with torch.no_grad():\n",
        "  tblock_test = TransformerBlock(d_model=D_MODEL, d_ffn=D_FFN, num_heads=NUM_HEADS, max_seqlen=SEQLEN)\n",
        "  input_act = torch.randn(BSZ, SEQLEN, D_MODEL)\n",
        "  tblock_out = tblock_test(input_act)\n",
        "  assert tblock_out.shape == (BSZ, SEQLEN, D_MODEL)\n",
        "\n",
        "  input_act_trunc = input_act[:,:2,:]\n",
        "  tblock_out2 = tblock_test(input_act_trunc)\n",
        "\n",
        "  assert torch.all(torch.isclose(tblock_out[0,0,:], tblock_out2[0,0,:]))\n",
        "  print(\"Logits match\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "plQSbdRDZRDC",
        "outputId": "ad1936f1-0f4f-4f5c-9951-0d97a073b7a1"
      },
      "execution_count": 274,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Logits match\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Dataloader"
      ],
      "metadata": {
        "id": "tuQgoyYXaPL2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "EOS_TOKEN=-1\n",
        "\n",
        "class DigitDataset(IterableDataset):\n",
        "  def __init__(self, num_digits=10):\n",
        "    self.num_digits = 10\n",
        "\n",
        "  def __iter__(self):\n",
        "    while True:\n",
        "      for i in range(self.num_digits):\n",
        "        yield i\n",
        "\n",
        "\n",
        "def collate_batch_digits(batch, seqlen, bsz):\n",
        "  x = torch.tensor(batch, dtype=torch.long).view(bsz, seqlen)\n",
        "  y = torch.roll(x, shifts=-1, dims=(1,))\n",
        "  y[:,-1] = EOS_TOKEN\n",
        "  return x,y\n",
        "\n",
        "# dataset that spits out 8 unsorted digits, then sorts them all\n",
        "\n",
        "class SortedDigitsDataset(IterableDataset):\n",
        "  # outputs a full batch\n",
        "  def __init__(self, num_digits=10, max_seqlen=8, eos_token=EOS_TOKEN):\n",
        "    self.num_digits = 10\n",
        "    self.max_seqlen = max_seqlen\n",
        "    self.eos_token = eos_token\n",
        "\n",
        "  def __iter__(self):\n",
        "    while True:\n",
        "      half = int(self.max_seqlen//2)\n",
        "      half_batch = torch.randint(low=0,high=(self.num_digits-1), size=(half,))\n",
        "      sorted = torch.sort(half_batch).values\n",
        "      batch = torch.cat([half_batch, sorted])\n",
        "      targets = torch.roll(batch, shifts=-1)\n",
        "      targets[:int(half)-1] = self.eos_token\n",
        "      targets[-1] = self.eos_token\n",
        "      #print(f\"batch:{batch}\\n targets:{targets}\\n\")\n",
        "      yield (batch, targets)\n"
      ],
      "metadata": {
        "id": "OZPMJ9VHG2na"
      },
      "execution_count": 275,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Dataloader test"
      ],
      "metadata": {
        "id": "fhA1W-c9fuxK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sorted_digits_dataset = SortedDigitsDataset(max_seqlen=SEQLEN)\n",
        "sorted_dataloader = DataLoader(dataset=sorted_digits_dataset, batch_size=BSZ, shuffle=False)\n",
        "\n",
        "an_iter = iter(sorted_digits_dataset)\n",
        "while True:\n",
        "  print(next(an_iter))\n",
        "  break"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KByqwSM1fxyz",
        "outputId": "6fd6ed55-7891-4e9d-9638-2ec0432be433"
      },
      "execution_count": 276,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(tensor([1, 4, 4, 6, 1, 7, 1, 5, 1, 1, 1, 4, 4, 5, 6, 7]), tensor([-1, -1, -1, -1, -1, -1, -1,  1,  1,  1,  4,  4,  5,  6,  7, -1]))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Constants"
      ],
      "metadata": {
        "id": "K6UBpVpR0w1T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "D_MODEL=16\n",
        "D_FFN=64\n",
        "NUM_LAYERS=2\n",
        "VOCAB_SIZE=16\n",
        "SEQLEN=16\n",
        "BSZ=32\n",
        "NUM_HEADS=4\n",
        "HEAD_DIM=D_MODEL//NUM_HEADS"
      ],
      "metadata": {
        "id": "iwsF3nnZ0wMK"
      },
      "execution_count": 277,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Dataloader"
      ],
      "metadata": {
        "id": "NR6aFw_S0z0P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sorted_digits_dataset = SortedDigitsDataset(max_seqlen=SEQLEN)\n",
        "sorted_dataloader = DataLoader(dataset=sorted_digits_dataset, batch_size=BSZ, shuffle=False)\n",
        "dl_iter = iter(sorted_dataloader)"
      ],
      "metadata": {
        "id": "isnsjgkh008a"
      },
      "execution_count": 278,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Trainer"
      ],
      "metadata": {
        "id": "Uxlg3n9kaR0O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "STEPS=5001\n",
        "\n",
        "model = Transformer(d_model=D_MODEL, d_ffn=D_FFN, vocab_size=VOCAB_SIZE, num_layers=NUM_LAYERS, num_heads=NUM_HEADS, max_seqlen=SEQLEN, simple_pos_embed=True)\n",
        "optimizer = torch.optim.AdamW(params=model.parameters(), lr=0.001, weight_decay=0)\n",
        "\n",
        "#digits = DigitDataset()\n",
        "#digit_dataloader = DataLoader(dataset=digits, batch_size=SEQLEN*BSZ, shuffle=False, collate_fn=lambda b:collate_batch_digits(batch=b, seqlen=SEQLEN, bsz=BSZ))\n",
        "#dl_iter = iter(digit_dataloader)\n",
        "\n",
        "\n",
        "for step in range(STEPS):\n",
        "  optimizer.zero_grad()\n",
        "  tokens, targets = next(dl_iter) #targets = (bsz, seqlen)   tokens=(bsz, seqlen)\n",
        "  logits = model(tokens)  # (bsz, seqlen, vocab_size)\n",
        "  vocab_size = logits.shape[-1]\n",
        "  loss = torch.nn.functional.cross_entropy(input=logits.view(-1, vocab_size), target=targets.view(-1), ignore_index=EOS_TOKEN)\n",
        "  if step %100 == 0:\n",
        "    print(f\"{step=} {loss=}\")\n",
        "    #print(f\"tokens:{tokens[0,:]} \\n targets:{targets[0,:]} \\n\\n\")\n",
        "    #print(f\"tokens:{tokens[0,:]} \\n targets:{targets[0,:]} \\n probs:\\n{logits[0,:,:]}\\n\")\n",
        "  loss.backward()\n",
        "  optimizer.step()\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ms1MEO7EGIDj",
        "outputId": "2cf834aa-77c7-48d8-a2e3-d22debfcf269"
      },
      "execution_count": 279,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/nn/init.py:295: UserWarning: mean is more than 2 std from [a, b] in nn.init.trunc_normal_. The distribution of values may be incorrect.\n",
            "  return _no_grad_trunc_normal_(tensor, mean, std, a, b, generator=generator)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step=0 loss=tensor(32.994, grad_fn=<NllLossBackward0>)\n",
            "step=100 loss=tensor(2.026, grad_fn=<NllLossBackward0>)\n",
            "step=200 loss=tensor(1.403, grad_fn=<NllLossBackward0>)\n",
            "step=300 loss=tensor(1.323, grad_fn=<NllLossBackward0>)\n",
            "step=400 loss=tensor(1.248, grad_fn=<NllLossBackward0>)\n",
            "step=500 loss=tensor(1.168, grad_fn=<NllLossBackward0>)\n",
            "step=600 loss=tensor(1.187, grad_fn=<NllLossBackward0>)\n",
            "step=700 loss=tensor(1.166, grad_fn=<NllLossBackward0>)\n",
            "step=800 loss=tensor(1.158, grad_fn=<NllLossBackward0>)\n",
            "step=900 loss=tensor(1.108, grad_fn=<NllLossBackward0>)\n",
            "step=1000 loss=tensor(1.106, grad_fn=<NllLossBackward0>)\n",
            "step=1100 loss=tensor(1.066, grad_fn=<NllLossBackward0>)\n",
            "step=1200 loss=tensor(1.047, grad_fn=<NllLossBackward0>)\n",
            "step=1300 loss=tensor(1.003, grad_fn=<NllLossBackward0>)\n",
            "step=1400 loss=tensor(0.988, grad_fn=<NllLossBackward0>)\n",
            "step=1500 loss=tensor(0.954, grad_fn=<NllLossBackward0>)\n",
            "step=1600 loss=tensor(0.940, grad_fn=<NllLossBackward0>)\n",
            "step=1700 loss=tensor(0.803, grad_fn=<NllLossBackward0>)\n",
            "step=1800 loss=tensor(0.747, grad_fn=<NllLossBackward0>)\n",
            "step=1900 loss=tensor(0.686, grad_fn=<NllLossBackward0>)\n",
            "step=2000 loss=tensor(0.646, grad_fn=<NllLossBackward0>)\n",
            "step=2100 loss=tensor(0.607, grad_fn=<NllLossBackward0>)\n",
            "step=2200 loss=tensor(0.543, grad_fn=<NllLossBackward0>)\n",
            "step=2300 loss=tensor(0.472, grad_fn=<NllLossBackward0>)\n",
            "step=2400 loss=tensor(0.454, grad_fn=<NllLossBackward0>)\n",
            "step=2500 loss=tensor(0.399, grad_fn=<NllLossBackward0>)\n",
            "step=2600 loss=tensor(0.328, grad_fn=<NllLossBackward0>)\n",
            "step=2700 loss=tensor(0.358, grad_fn=<NllLossBackward0>)\n",
            "step=2800 loss=tensor(0.320, grad_fn=<NllLossBackward0>)\n",
            "step=2900 loss=tensor(0.265, grad_fn=<NllLossBackward0>)\n",
            "step=3000 loss=tensor(0.287, grad_fn=<NllLossBackward0>)\n",
            "step=3100 loss=tensor(0.210, grad_fn=<NllLossBackward0>)\n",
            "step=3200 loss=tensor(0.189, grad_fn=<NllLossBackward0>)\n",
            "step=3300 loss=tensor(0.254, grad_fn=<NllLossBackward0>)\n",
            "step=3400 loss=tensor(0.222, grad_fn=<NllLossBackward0>)\n",
            "step=3500 loss=tensor(0.128, grad_fn=<NllLossBackward0>)\n",
            "step=3600 loss=tensor(0.116, grad_fn=<NllLossBackward0>)\n",
            "step=3700 loss=tensor(0.097, grad_fn=<NllLossBackward0>)\n",
            "step=3800 loss=tensor(0.113, grad_fn=<NllLossBackward0>)\n",
            "step=3900 loss=tensor(0.087, grad_fn=<NllLossBackward0>)\n",
            "step=4000 loss=tensor(0.060, grad_fn=<NllLossBackward0>)\n",
            "step=4100 loss=tensor(0.077, grad_fn=<NllLossBackward0>)\n",
            "step=4200 loss=tensor(0.046, grad_fn=<NllLossBackward0>)\n",
            "step=4300 loss=tensor(0.044, grad_fn=<NllLossBackward0>)\n",
            "step=4400 loss=tensor(0.041, grad_fn=<NllLossBackward0>)\n",
            "step=4500 loss=tensor(0.088, grad_fn=<NllLossBackward0>)\n",
            "step=4600 loss=tensor(0.030, grad_fn=<NllLossBackward0>)\n",
            "step=4700 loss=tensor(0.044, grad_fn=<NllLossBackward0>)\n",
            "step=4800 loss=tensor(0.058, grad_fn=<NllLossBackward0>)\n",
            "step=4900 loss=tensor(0.033, grad_fn=<NllLossBackward0>)\n",
            "step=5000 loss=tensor(0.035, grad_fn=<NllLossBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# generation\n",
        "# dumb way: 1 token at a time. get logits, do softmax, sample token. feed in full generated thing back\n",
        "\n",
        "# kv cache way: attention takes in a kv cache which are pre-computed k and v values for previous tokens\n",
        "\n"
      ],
      "metadata": {
        "id": "0FbHrEmhMayx"
      },
      "execution_count": 280,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Simple generation"
      ],
      "metadata": {
        "id": "TYAurIkmqA74"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def softmax_with_temp_last_dim(logits: torch.Tensor, temp: float=1.0, eps=1e-6):\n",
        "  # softmax across the innermost (last) dimension\n",
        "  # compute max\n",
        "  # subtract max\n",
        "  # numerator = exponentiate with temperature\n",
        "  # denominator = sum of numerator\n",
        "  #import pdb; pdb.set_trace()\n",
        "  # logits: (..., vocab_size)\n",
        "  maxval = torch.amax(logits, dim=-1).unsqueeze(-1)\n",
        "  max_subtracted_logits = logits - maxval\n",
        "  scaled_max_subtracted_logits = max_subtracted_logits/torch.Tensor([temp+eps])\n",
        "  numerator = torch.exp(scaled_max_subtracted_logits)\n",
        "  denominator = torch.sum(numerator, dim=-1).unsqueeze(-1)\n",
        "  return numerator/(denominator + eps)\n",
        "\n",
        "\n",
        "def retain_top_p(probs: torch.Tensor, p: float):\n",
        "  # retain only probabilities with mass = top_p\n",
        "  assert len(probs.shape) == 1 # batch later\n",
        "  sorted_probs = torch.sort(probs, descending=True)\n",
        "  sorted_probs_values = sorted_probs.values\n",
        "  sorted_probs_indices = sorted_probs.indices\n",
        "  cumsum_probs = list(torch.cumsum(sorted_probs_values, dim=0))\n",
        "  for i, prob in enumerate(cumsum_probs):  # super ugly, need vectorized version\n",
        "    if prob > p:\n",
        "      break\n",
        "  not_useful_indices = sorted_probs_indices[(i+1):]\n",
        "  if len(not_useful_indices) > 0:\n",
        "    probs[not_useful_indices] = 0 # TODO check here\n",
        "  return probs\n",
        "\n",
        "\n",
        "\n",
        "def sample(logits: torch.Tensor, top_p: Optional[float]=None, temp: float = 1.0):\n",
        "  # logits: (vocab_size,)\n",
        "  probs = softmax_with_temp_last_dim(logits=logits, temp=temp)\n",
        "  print(probs)\n",
        "  if top_p:\n",
        "    probs = retain_top_p(probs=probs, p=top_p)\n",
        "  sampled = np.random.multinomial(n=1, pvals=probs)\n",
        "  return int(np.argwhere(sampled)[0][0])\n",
        "\n",
        "\n",
        "def generate(the_model: torch.nn.Module, input_tokens: torch.Tensor, toks_to_generate: int, temp=1.0):\n",
        "  # for each token, run forward\n",
        "    # run forward get logits\n",
        "    # convert logits to probabilities\n",
        "    # sample from probabilities\n",
        "    # update input tokens\n",
        "  generated_toks = []\n",
        "  for _ in range(toks_to_generate):\n",
        "    logits = the_model(input_tokens)\n",
        "    #print(f\"logits.shape:{logits.shape}\")\n",
        "    #print(f\"full_logits:{logits}\")\n",
        "    logits = logits.squeeze(0)[-1,:] #unsqueeze to remove batch dim. -1 for last token\n",
        "    #print(f\"logits:{logits}\")\n",
        "    sampled_token = sample(logits, temp=temp)\n",
        "    generated_toks.append(sampled_token)\n",
        "    #print(input_tokens.shape)\n",
        "    #print(torch.tensor([sampled_token]).unsqueeze(0).shape)\n",
        "    input_tokens = torch.cat([input_tokens.squeeze(0), torch.tensor([sampled_token])]).unsqueeze(0)\n",
        "  return generated_toks\n",
        ""
      ],
      "metadata": {
        "id": "rtQTtV_sqCSL"
      },
      "execution_count": 286,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Simple tests"
      ],
      "metadata": {
        "id": "PVQertKT70yf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "input, targets = next(dl_iter) #get the first entry only\n",
        "input = input[:1,:]\n",
        "targets = targets[:1,:]\n",
        "print(input)\n",
        "print(targets)\n",
        "trunc_input = input[:1,:int(SEQLEN//2)]\n",
        "print(trunc_input)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nlqqXCBI51Wy",
        "outputId": "433b2880-88ff-4ed8-c4de-912f0f167eb9"
      },
      "execution_count": 282,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[2, 6, 5, 1, 8, 7, 8, 8, 1, 2, 5, 6, 7, 8, 8, 8]])\n",
            "tensor([[-1, -1, -1, -1, -1, -1, -1,  1,  2,  5,  6,  7,  8,  8,  8, -1]])\n",
            "tensor([[2, 6, 5, 1, 8, 7, 8, 8]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "logits_raw = model(input)\n",
        "print(f\"raw_logits:{logits_raw}\")\n",
        "print(f\"logits for last:{logits_raw.squeeze(0)[7,:]}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tLKWEyqo6ydq",
        "outputId": "3b1581a6-1152-405f-b105-645df9fef3a2"
      },
      "execution_count": 287,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "raw_logits:tensor([[[ 9.250e-01,  6.369e+00,  4.578e+01,  1.682e+01,  1.213e+00, -3.247e+01, -2.743e+01, -5.083e+01, -3.037e+01, -4.624e+01, -1.019e+01, -8.336e+00, -2.496e+01, -3.601e+01, -5.231e+00, -3.079e+01],\n",
            "         [ 7.895e-02, -4.665e+00, -8.957e-01, -1.325e+01,  7.796e+00,  3.988e+00,  1.104e+01, -1.361e+01, -6.082e+00, -2.768e+01, -1.301e+01, -1.166e+01, -2.218e+01, -9.998e+00, -8.620e+00, -3.163e+01],\n",
            "         [-1.834e+01, -3.757e+00,  3.879e+00,  2.956e-01, -6.140e+00,  1.164e+01,  1.186e+01, -1.313e+01, -1.888e+01, -2.251e+01, -9.471e+00, -1.815e+01, -2.338e+01, -6.518e+00, -1.831e+01, -3.042e+01],\n",
            "         [ 3.009e+00, -8.810e+00,  1.465e+01,  9.414e+00,  3.503e+00, -1.591e+01, -4.405e+00, -1.555e+01,  1.195e+00, -1.322e+01, -9.820e+00, -1.963e+00,  4.824e+00, -9.279e+00, -2.240e+00, -6.977e+00],\n",
            "         [ 1.902e+01,  3.678e+01,  3.548e+01,  9.157e+00,  4.421e+00, -2.037e+01, -4.308e+01, -5.010e+01, -3.421e+01, -4.400e+01, -1.350e+01, -1.097e+01, -3.653e+01, -2.887e+01, -1.793e+01, -3.884e+01],\n",
            "         [-2.060e+01, -7.820e+00, -1.854e+01, -9.730e+00,  8.104e+00,  8.804e+00,  2.061e+01,  6.629e+01,  5.929e+01, -3.324e-01, -1.531e+01, -4.915e+00,  7.301e+00, -2.236e+00,  1.115e+01,  1.735e+01],\n",
            "         [-1.776e+00,  7.283e+00,  3.370e+00, -2.454e+00,  1.583e+01, -9.288e-01, -1.011e+01,  1.880e+01,  1.574e+01, -5.476e+00, -2.349e+00, -4.887e+00, -1.580e+00, -5.016e+00,  2.633e+00,  2.082e+00],\n",
            "         [ 1.820e+01,  3.581e+01,  2.398e+01,  4.927e+00,  1.320e+01, -4.274e+00, -4.329e+01, -3.174e+01, -1.899e+01, -2.791e+01, -4.632e+00, -1.035e+01, -2.570e+01, -1.363e+01, -1.326e+01, -2.857e+01],\n",
            "         [-2.962e+00,  1.287e+01,  2.191e+01,  1.016e+01,  3.974e+00, -6.334e+00, -2.739e+01, -4.625e+01, -3.144e+01, -4.455e+01, -1.438e+01, -2.286e+01, -3.311e+01, -1.760e+01, -2.383e+01, -3.892e+01],\n",
            "         [-1.995e+01, -5.877e+00, -9.061e+00, -6.661e+00,  5.628e+00,  1.294e+01,  7.292e+00, -1.431e+01, -2.016e+01, -3.410e+01, -1.634e+01, -2.250e+01, -3.273e+01, -5.669e+00, -1.965e+01, -3.954e+01],\n",
            "         [-7.687e+00, -3.036e+00, -2.678e+01, -2.758e+01,  1.700e+00,  2.453e+01,  3.057e+01,  2.012e+01,  1.431e+01, -1.355e+01, -1.397e+01, -1.581e+01, -2.306e+01, -1.685e+00, -6.985e+00, -1.987e+01],\n",
            "         [-9.481e+00, -1.148e+01, -2.825e+01, -2.524e+01, -1.675e+00,  1.487e+01,  4.077e+01,  4.535e+01,  4.072e+01, -1.207e+01, -1.927e+01, -1.143e+01, -1.476e+01, -5.209e+00,  3.714e+00, -4.099e+00],\n",
            "         [-7.399e+00, -1.326e+01, -2.647e+01, -2.086e+01,  5.526e+00,  1.015e+01,  3.284e+01,  5.417e+01,  5.977e+01, -1.162e+01, -2.578e+01, -8.526e+00, -6.785e+00, -5.093e+00,  4.868e+00,  7.881e-01],\n",
            "         [-1.497e+01, -1.431e+01, -2.023e+01, -2.219e+01,  2.580e+00,  8.975e+00,  3.003e+01,  4.738e+01,  7.225e+01, -2.004e+01, -2.504e+01, -1.395e+01, -6.269e+00, -1.112e+01,  4.710e+00,  2.271e+00],\n",
            "         [-1.260e+01, -1.113e+01, -1.195e+01, -2.269e+01,  6.467e-02,  7.970e+00,  2.539e+01,  3.986e+01,  7.574e+01, -1.851e+01, -1.821e+01, -4.469e+00, -1.169e-01, -1.198e+01,  1.116e+01,  3.322e+00],\n",
            "         [-1.074e+01, -1.003e+01, -7.319e+00, -1.368e+01,  1.031e+01, -4.794e+00,  2.327e+01,  4.108e+01,  4.869e+01, -2.062e+01, -2.161e+01, -2.793e+00, -7.938e-01, -1.221e+01,  1.138e+01,  1.174e+00]]], grad_fn=<ViewBackward0>)\n",
            "logits for last:tensor([ 18.196,  35.810,  23.980,   4.927,  13.196,  -4.274, -43.287, -31.740, -18.986, -27.908,  -4.632, -10.346, -25.701, -13.626, -13.258, -28.570], grad_fn=<SliceBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# test for sampling functions\n",
        "#logits = torch.tensor([5,1,1,5,1])\n",
        "#sample(logits, temp=0)\n",
        "\n",
        "with torch.no_grad():\n",
        "  # test for full generation\n",
        "  #input = next(dl_iter)[0][0,:].unsqueeze(0) #get the first entry only\n",
        "  generated = generate(the_model=model, input_tokens=trunc_input, toks_to_generate=8, temp=1)\n",
        "  print(generated)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-shFNf4v7z5A",
        "outputId": "f34de21c-a2cf-41ca-8147-0936db9c225f"
      },
      "execution_count": 288,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([2.242e-08, 1.000e+00, 7.289e-06, 3.873e-14, 1.510e-10, 3.906e-18, 4.456e-35, 4.610e-30, 1.594e-24, 2.127e-28, 2.732e-18, 9.010e-21, 1.933e-27, 3.393e-22, 4.898e-22, 1.098e-28])\n",
            "tensor([1.572e-11, 1.179e-04, 9.999e-01, 7.856e-06, 1.618e-08, 5.396e-13, 3.864e-22, 2.487e-30, 6.742e-24, 1.368e-29, 1.723e-16, 3.572e-20, 1.266e-24, 6.884e-18, 1.361e-20, 3.796e-27])\n",
            "tensor([5.205e-15, 6.702e-09, 2.777e-10, 3.062e-09, 6.653e-04, 9.958e-01, 3.512e-03, 1.455e-12, 4.218e-15, 3.700e-21, 1.910e-13, 4.051e-16, 1.464e-20, 8.254e-09, 6.992e-15, 1.612e-23])\n",
            "tensor([2.432e-17, 2.546e-15, 1.240e-25, 5.567e-26, 2.902e-13, 2.387e-03, 9.976e-01, 2.896e-05, 8.656e-08, 6.944e-20, 4.537e-20, 7.240e-21, 5.129e-24, 9.839e-15, 4.911e-17, 1.243e-22])\n",
            "tensor([1.509e-24, 2.053e-25, 1.064e-32, 2.172e-31, 3.706e-21, 5.678e-14, 1.005e-02, 9.803e-01, 9.612e-03, 1.132e-25, 8.430e-29, 2.143e-25, 7.663e-27, 1.082e-22, 8.116e-19, 3.283e-22])\n",
            "tensor([6.718e-30, 1.919e-32, 3.496e-38, 9.600e-36, 2.757e-24, 2.823e-22, 2.008e-12, 3.699e-03, 9.963e-01, 9.855e-32, 7.011e-38, 2.177e-30, 1.242e-29, 6.740e-29, 1.428e-24, 2.415e-26])\n",
            "tensor([1.323e-38, 2.568e-38, 6.885e-41, 9.666e-42, 5.539e-31, 3.317e-28, 4.614e-19, 1.585e-11, 1.000e+00, 8.281e-41, 5.619e-43, 3.651e-38, 7.948e-35, 6.228e-37, 4.660e-30, 4.067e-31])\n",
            "tensor([4.330e-39, 1.875e-38, 8.330e-39, 1.794e-43, 1.370e-33, 3.714e-30, 1.367e-22, 2.621e-16, 1.000e+00, 1.179e-41, 1.589e-41, 1.471e-35, 1.143e-33, 8.034e-39, 9.023e-29, 3.559e-32])\n",
            "[1, 2, 5, 6, 7, 8, 8, 8]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Sample"
      ],
      "metadata": {
        "id": "CVPCOS8kEa2R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "probs"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YDpViMId8pqe",
        "outputId": "96410a6e-67b4-40cd-8b34-908d8c9fe63e"
      },
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([0.487, 0.009, 0.009, 0.487, 0.009])"
            ]
          },
          "metadata": {},
          "execution_count": 62
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "int(np.argwhere(np.random.multinomial(n=1, pvals=probs))[0][0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sI8RXUbQBFTY",
        "outputId": "ce77aa01-90aa-4dab-d7d4-09b077f10981"
      },
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "3"
            ]
          },
          "metadata": {},
          "execution_count": 57
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KQDNdkfOCUfy",
        "outputId": "9aa07fef-7735-422c-81d1-46b72a2614ce"
      },
      "execution_count": 133,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Transformer(\n",
              "  (embedding): Embedding(16, 16)\n",
              "  (pos_embedding): Embedding(16, 16)\n",
              "  (output_layer): Linear(in_features=16, out_features=16, bias=True)\n",
              "  (layers): ModuleList(\n",
              "    (0-1): 2 x TransformerBlock(\n",
              "      (mlp_norm): RMSNorm((16,), eps=None, elementwise_affine=True)\n",
              "      (mlp): MLP(\n",
              "        (fc1): Linear(in_features=16, out_features=64, bias=False)\n",
              "        (fc2): Linear(in_features=64, out_features=16, bias=False)\n",
              "      )\n",
              "      (attn_norm): RMSNorm((16,), eps=None, elementwise_affine=True)\n",
              "      (attn): MultiHeadAttention(\n",
              "        (wqkv): Linear(in_features=16, out_features=48, bias=False)\n",
              "        (wo): Linear(in_features=16, out_features=16, bias=True)\n",
              "      )\n",
              "    )\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 133
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "aLwixnft04Bb"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}