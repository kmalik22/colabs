{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPoBvY9peULWBGZJGF3Zxyx",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kmalik22/colabs/blob/main/transformer_training_pytorch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 86,
      "metadata": {
        "id": "xKr0v3brEOJW"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import torch.nn.functional as F\n",
        "import itertools\n",
        "from typing import Optional\n",
        "from torch.utils.data import IterableDataset, DataLoader\n",
        "torch.set_printoptions(linewidth=250, precision=3)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Modules"
      ],
      "metadata": {
        "id": "FkrwnUY_aTuI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MLP(torch.nn.Module):\n",
        "  def __init__(self, d_model, d_ffn):\n",
        "    super().__init__()\n",
        "    self.d_model = d_model\n",
        "    self.d_ffn = d_ffn\n",
        "    self.fc1 = torch.nn.Linear(in_features=d_model, out_features=d_ffn, bias=False)\n",
        "    #torch.nn.init.trunc_normal_(self.fc1.weight, mean=0, std=1/d_model**0.5)\n",
        "    self.fc2 = torch.nn.Linear(in_features=d_ffn, out_features=d_model, bias=False)\n",
        "    #torch.nn.init.trunc_normal_(self.fc2.weight, mean=0, std=1/d_ffn**0.5, a=-3, b=3)\n",
        "\n",
        "  def forward(self, x: torch.Tensor):\n",
        "    fc1_out = self.fc1(x)\n",
        "    relu_out = torch.nn.functional.relu(fc1_out)\n",
        "    fc2_out = self.fc2(relu_out)\n",
        "    return fc2_out\n",
        "\n",
        "class MultiHeadAttention(torch.nn.Module):\n",
        "  def __init__(self, d_model, num_heads):\n",
        "    super().__init__()\n",
        "    self.d_model = d_model\n",
        "    self.num_heads = num_heads\n",
        "    self.head_dim = d_model//num_heads\n",
        "    self.wqkv = torch.nn.Linear(d_model, int(3*d_model), bias=False)\n",
        "    torch.nn.init.trunc_normal_(self.wqkv.weight, 0, 1/d_model**0.5, a=-3, b=3)\n",
        "    self.wo = torch.nn.Linear(d_model, d_model)\n",
        "    torch.nn.init.trunc_normal_(self.wo.weight, 0, 1/d_model**0.5, a=-3, b=3)\n",
        "    # TODO: add rope\n",
        "\n",
        "  def scaled_dot_product_attn(self, q, k, v):\n",
        "    # q, k, v = (bsz, num_heads, seqlen, head_dim)\n",
        "    # qk.t()/sqrt(head_dim)\n",
        "    # causal mask\n",
        "    # softmax\n",
        "    # @ v\n",
        "    seqlen = q.shape[2]\n",
        "    attn_wts = q @ k.transpose(2,3)  #(bsz,num_h,seqlen,head_dim) (bsz,num_h,head_dim,seqlen) -> (bsz,num_h,seqlen,seqlen)\n",
        "    # create mask, do torch.where. 1=use attn wts, else use -inf\n",
        "    mask = torch.tril(torch.ones(seqlen,seqlen)).to(torch.bool)\n",
        "    masked_attn_wts = torch.where(mask, attn_wts, float('-inf')) # (bsz,num_heads,seqlen,seqlen)\n",
        "    # softmax TODO check once\n",
        "    softmax_wts = torch.nn.functional.softmax(masked_attn_wts,dim=-1)\n",
        "    # (bsz,num_heads,seqlen,seqlen) @ (bsz,num_heasds,seqlen,head_dim) -> (bsz,num_heads,seqlen,head_dim)\n",
        "    return softmax_wts @ v\n",
        "\n",
        "\n",
        "  def forward(self, x:torch.Tensor):\n",
        "    #x = (bsz, seqlen, d_model)\n",
        "    bsz, seqlen, d_model = x.shape\n",
        "    assert d_model == self.d_model\n",
        "    wqkv_out = self.wqkv(x) # (bsz, seqlen, 3*d_model)\n",
        "    # rearrange so seqlen,head_dim are the last two dims\n",
        "    wqkv_out = wqkv_out.reshape(bsz, 3, self.num_heads, seqlen, self.head_dim)\n",
        "    q = wqkv_out[:,0,:,:,:] # (bsz, num_heads, seqlen, head_dim)\n",
        "    k = wqkv_out[:,1,:,:,:] # (bsz, num_heads, seqlen, head_dim)\n",
        "    v = wqkv_out[:,2,:,:,:] # (bsz, num_heads, seqlen, head_dim)\n",
        "    # TODO: add rope to q and k\n",
        "    self_attn_out = self.scaled_dot_product_attn(q, k, v) #(bsz, num_heads, seqlen, head_dim)\n",
        "    # transpose\n",
        "    self_attn_out = self_attn_out.transpose(1,2) # (bsz,seqlen,num_heads,head_dim)\n",
        "    # concat the last dim\n",
        "    self_attn_out = self_attn_out.reshape(bsz,seqlen,-1)\n",
        "    # wo\n",
        "    self_attn_out = self_attn_out.reshape(bsz, seqlen, d_model)\n",
        "    wo_out = self.wo(self_attn_out)\n",
        "    return wo_out\n",
        "\n",
        "\n",
        "class TransformerBlock(torch.nn.Module):\n",
        "  def __init__(self, d_model, d_ffn, num_heads, max_seqlen):\n",
        "    super().__init__()\n",
        "    self.mlp_norm = torch.nn.RMSNorm(normalized_shape=d_model)\n",
        "    self.mlp = MLP(d_model=d_model, d_ffn=d_ffn)\n",
        "    self.attn_norm = torch.nn.RMSNorm(normalized_shape=d_model)\n",
        "    self.attn = MultiHeadAttention(d_model=d_model,num_heads=num_heads)\n",
        "\n",
        "\n",
        "  def forward(self, x:torch.Tensor):\n",
        "    # norm --> attn, add back to original --> norm --> mlp, add back\n",
        "    attn_norm_out = self.attn_norm(x)\n",
        "    attn_out = self.attn(attn_norm_out)\n",
        "    attn_out_post_resid = x + attn_out\n",
        "    mlp_norm_out = self.mlp_norm(attn_out_post_resid)\n",
        "    mlp_out = self.mlp(mlp_norm_out)\n",
        "    return mlp_out + attn_out_post_resid\n",
        "\n",
        "class Transformer(torch.nn.Module):\n",
        "  def __init__(self, d_model, d_ffn, vocab_size, num_layers, max_seqlen, num_heads, simple_pos_embed=False):\n",
        "    super().__init__()\n",
        "    self.max_seqlen = max_seqlen\n",
        "    self.embedding = torch.nn.Embedding(num_embeddings=vocab_size, embedding_dim=d_model)\n",
        "    torch.nn.init.trunc_normal_(self.embedding.weight, mean=0, std=1/d_model**0.5, a=-3, b=-3)\n",
        "\n",
        "    self.simple_pos_embed = simple_pos_embed\n",
        "    if simple_pos_embed:\n",
        "      self.pos_embedding = torch.nn.Embedding(num_embeddings=max_seqlen, embedding_dim=d_model)\n",
        "\n",
        "    self.output_layer = torch.nn.Linear(in_features=d_model, out_features=vocab_size)\n",
        "    torch.nn.init.trunc_normal_(self.output_layer.weight, mean=0, std=1/d_model**0.5, a=-3, b=3)\n",
        "    self.layers = torch.nn.ModuleList(\n",
        "        [TransformerBlock(d_model=d_model, d_ffn=d_ffn, num_heads=num_heads, max_seqlen=max_seqlen) for _ in range(num_layers)]\n",
        "    )\n",
        "    # TODO: do proper init\n",
        "    # add attention\n",
        "    # add layer norms\n",
        "\n",
        "  def forward(self, tokens: torch.Tensor):\n",
        "    curr_out = self.embedding(tokens)\n",
        "    if self.simple_pos_embed:\n",
        "      curr_out = curr_out + self.pos_embedding(torch.arange(self.max_seqlen))\n",
        "    for l in self.layers:\n",
        "      curr_out = curr_out + l(curr_out)\n",
        "    logits = self.output_layer(curr_out)  # bsz,seqlen,vocab_size\n",
        "    return logits\n"
      ],
      "metadata": {
        "id": "KgGPaK6cERnA"
      },
      "execution_count": 210,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Test code"
      ],
      "metadata": {
        "id": "J3MNY6h3aQUh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tblock_test = TransformerBlock(d_model=D_MODEL, d_ffn=D_FFN, num_heads=NUM_HEADS, max_seqlen=SEQLEN)\n",
        "input_act = torch.randn(BSZ, SEQLEN, D_MODEL)\n",
        "tblock_out = tblock_test(input_act)\n",
        "assert tblock_out.shape == (BSZ, SEQLEN, D_MODEL)"
      ],
      "metadata": {
        "id": "plQSbdRDZRDC"
      },
      "execution_count": 211,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Dataloader"
      ],
      "metadata": {
        "id": "tuQgoyYXaPL2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "EOS_TOKEN=-1\n",
        "\n",
        "class DigitDataset(IterableDataset):\n",
        "  def __init__(self, num_digits=10):\n",
        "    self.num_digits = 10\n",
        "\n",
        "  def __iter__(self):\n",
        "    while True:\n",
        "      for i in range(self.num_digits):\n",
        "        yield i\n",
        "\n",
        "\n",
        "def collate_batch_digits(batch, seqlen, bsz):\n",
        "  x = torch.tensor(batch, dtype=torch.long).view(bsz, seqlen)\n",
        "  y = torch.roll(x, shifts=-1, dims=(1,))\n",
        "  y[:,-1] = EOS_TOKEN\n",
        "  return x,y\n",
        "\n",
        "# dataset that spits out 8 unsorted digits, then sorts them all\n",
        "\n",
        "class SortedDigitsDataset(IterableDataset):\n",
        "  # outputs a full batch\n",
        "  def __init__(self, num_digits=10, max_seqlen=8, eos_token=EOS_TOKEN):\n",
        "    self.num_digits = 10\n",
        "    self.max_seqlen = max_seqlen\n",
        "    self.eos_token = eos_token\n",
        "\n",
        "  def __iter__(self):\n",
        "    while True:\n",
        "      half = int(self.max_seqlen//2)\n",
        "      half_batch = torch.randint(low=0,high=(self.num_digits-1), size=(half,))\n",
        "      sorted = torch.sort(half_batch).values\n",
        "      batch = torch.cat([half_batch, sorted])\n",
        "      targets = torch.roll(batch, shifts=-1)\n",
        "      targets[:int(half)-1] = self.eos_token\n",
        "      targets[-1] = self.eos_token\n",
        "      #print(f\"batch:{batch}\\n targets:{targets}\\n\")\n",
        "      yield (batch, targets)\n"
      ],
      "metadata": {
        "id": "OZPMJ9VHG2na"
      },
      "execution_count": 212,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Dataloader test"
      ],
      "metadata": {
        "id": "fhA1W-c9fuxK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sorted_digits_dataset = SortedDigitsDataset(max_seqlen=SEQLEN)\n",
        "sorted_dataloader = DataLoader(dataset=sorted_digits_dataset, batch_size=BSZ, shuffle=False)\n",
        "\n",
        "an_iter = iter(sorted_digits_dataset)\n",
        "while True:\n",
        "  print(next(an_iter))\n",
        "  break"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KByqwSM1fxyz",
        "outputId": "76d4555b-f277-45ba-8f1a-12a9f886da2c"
      },
      "execution_count": 202,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "batch:tensor([8, 1, 8, 3, 2, 0, 3, 1, 0, 1, 1, 2, 3, 3, 8, 8])\n",
            " targets:tensor([-1, -1, -1, -1, -1, -1, -1,  0,  1,  1,  2,  3,  3,  8,  8, -1])\n",
            "\n",
            "(tensor([8, 1, 8, 3, 2, 0, 3, 1, 0, 1, 1, 2, 3, 3, 8, 8]), tensor([-1, -1, -1, -1, -1, -1, -1,  0,  1,  1,  2,  3,  3,  8,  8, -1]))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Trainer"
      ],
      "metadata": {
        "id": "Uxlg3n9kaR0O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "STEPS=5001\n",
        "D_MODEL=16\n",
        "D_FFN=64\n",
        "NUM_LAYERS=2\n",
        "VOCAB_SIZE=16\n",
        "SEQLEN=16\n",
        "BSZ=32\n",
        "NUM_HEADS=4\n",
        "HEAD_DIM=D_MODEL//NUM_HEADS\n",
        "\n",
        "model = Transformer(d_model=D_MODEL, d_ffn=D_FFN, vocab_size=VOCAB_SIZE, num_layers=NUM_LAYERS, num_heads=NUM_HEADS, max_seqlen=SEQLEN, simple_pos_embed=True)\n",
        "optimizer = torch.optim.AdamW(params=model.parameters(), lr=0.001, weight_decay=0)\n",
        "\n",
        "#digits = DigitDataset()\n",
        "#digit_dataloader = DataLoader(dataset=digits, batch_size=SEQLEN*BSZ, shuffle=False, collate_fn=lambda b:collate_batch_digits(batch=b, seqlen=SEQLEN, bsz=BSZ))\n",
        "#dl_iter = iter(digit_dataloader)\n",
        "\n",
        "\n",
        "sorted_digits_dataset = SortedDigitsDataset(max_seqlen=SEQLEN)\n",
        "sorted_dataloader = DataLoader(dataset=sorted_digits_dataset, batch_size=BSZ, shuffle=False)\n",
        "dl_iter = iter(sorted_dataloader)\n",
        "\n",
        "for step in range(STEPS):\n",
        "  optimizer.zero_grad()\n",
        "  tokens, targets = next(dl_iter) #targets = (bsz, seqlen)   tokens=(bsz, seqlen)\n",
        "  logits = model(tokens)  # (bsz, seqlen, vocab_size)\n",
        "  vocab_size = logits.shape[-1]\n",
        "  loss = torch.nn.functional.cross_entropy(input=logits.view(-1, vocab_size), target=targets.view(-1), ignore_index=EOS_TOKEN)\n",
        "  if step %100 == 0:\n",
        "    print(f\"{step=} {loss=}\")\n",
        "    #print(f\"tokens:{tokens[0,:]} \\n targets:{targets[0,:]} \\n\\n\")\n",
        "    #print(f\"tokens:{tokens[0,:]} \\n targets:{targets[0,:]} \\n probs:\\n{logits[0,:,:]}\\n\")\n",
        "  loss.backward()\n",
        "  optimizer.step()\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ms1MEO7EGIDj",
        "outputId": "a31e1387-46d2-4d11-a88b-50a162ed481e"
      },
      "execution_count": 217,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step=0 loss=tensor(26.004, grad_fn=<NllLossBackward0>)\n",
            "step=100 loss=tensor(1.815, grad_fn=<NllLossBackward0>)\n",
            "step=200 loss=tensor(1.312, grad_fn=<NllLossBackward0>)\n",
            "step=300 loss=tensor(1.234, grad_fn=<NllLossBackward0>)\n",
            "step=400 loss=tensor(1.178, grad_fn=<NllLossBackward0>)\n",
            "step=500 loss=tensor(1.153, grad_fn=<NllLossBackward0>)\n",
            "step=600 loss=tensor(1.126, grad_fn=<NllLossBackward0>)\n",
            "step=700 loss=tensor(1.037, grad_fn=<NllLossBackward0>)\n",
            "step=800 loss=tensor(0.958, grad_fn=<NllLossBackward0>)\n",
            "step=900 loss=tensor(0.934, grad_fn=<NllLossBackward0>)\n",
            "step=1000 loss=tensor(0.911, grad_fn=<NllLossBackward0>)\n",
            "step=1100 loss=tensor(0.839, grad_fn=<NllLossBackward0>)\n",
            "step=1200 loss=tensor(0.855, grad_fn=<NllLossBackward0>)\n",
            "step=1300 loss=tensor(0.766, grad_fn=<NllLossBackward0>)\n",
            "step=1400 loss=tensor(0.700, grad_fn=<NllLossBackward0>)\n",
            "step=1500 loss=tensor(0.612, grad_fn=<NllLossBackward0>)\n",
            "step=1600 loss=tensor(0.513, grad_fn=<NllLossBackward0>)\n",
            "step=1700 loss=tensor(0.399, grad_fn=<NllLossBackward0>)\n",
            "step=1800 loss=tensor(0.336, grad_fn=<NllLossBackward0>)\n",
            "step=1900 loss=tensor(0.270, grad_fn=<NllLossBackward0>)\n",
            "step=2000 loss=tensor(0.132, grad_fn=<NllLossBackward0>)\n",
            "step=2100 loss=tensor(0.112, grad_fn=<NllLossBackward0>)\n",
            "step=2200 loss=tensor(0.117, grad_fn=<NllLossBackward0>)\n",
            "step=2300 loss=tensor(0.122, grad_fn=<NllLossBackward0>)\n",
            "step=2400 loss=tensor(0.087, grad_fn=<NllLossBackward0>)\n",
            "step=2500 loss=tensor(0.043, grad_fn=<NllLossBackward0>)\n",
            "step=2600 loss=tensor(0.048, grad_fn=<NllLossBackward0>)\n",
            "step=2700 loss=tensor(0.070, grad_fn=<NllLossBackward0>)\n",
            "step=2800 loss=tensor(0.045, grad_fn=<NllLossBackward0>)\n",
            "step=2900 loss=tensor(0.020, grad_fn=<NllLossBackward0>)\n",
            "step=3000 loss=tensor(0.010, grad_fn=<NllLossBackward0>)\n",
            "step=3100 loss=tensor(0.023, grad_fn=<NllLossBackward0>)\n",
            "step=3200 loss=tensor(0.033, grad_fn=<NllLossBackward0>)\n",
            "step=3300 loss=tensor(0.027, grad_fn=<NllLossBackward0>)\n",
            "step=3400 loss=tensor(0.018, grad_fn=<NllLossBackward0>)\n",
            "step=3500 loss=tensor(0.025, grad_fn=<NllLossBackward0>)\n",
            "step=3600 loss=tensor(0.019, grad_fn=<NllLossBackward0>)\n",
            "step=3700 loss=tensor(0.016, grad_fn=<NllLossBackward0>)\n",
            "step=3800 loss=tensor(0.021, grad_fn=<NllLossBackward0>)\n",
            "step=3900 loss=tensor(0.029, grad_fn=<NllLossBackward0>)\n",
            "step=4000 loss=tensor(0.094, grad_fn=<NllLossBackward0>)\n",
            "step=4100 loss=tensor(0.015, grad_fn=<NllLossBackward0>)\n",
            "step=4200 loss=tensor(0.012, grad_fn=<NllLossBackward0>)\n",
            "step=4300 loss=tensor(0.014, grad_fn=<NllLossBackward0>)\n",
            "step=4400 loss=tensor(0.010, grad_fn=<NllLossBackward0>)\n",
            "step=4500 loss=tensor(0.040, grad_fn=<NllLossBackward0>)\n",
            "step=4600 loss=tensor(0.015, grad_fn=<NllLossBackward0>)\n",
            "step=4700 loss=tensor(0.007, grad_fn=<NllLossBackward0>)\n",
            "step=4800 loss=tensor(0.006, grad_fn=<NllLossBackward0>)\n",
            "step=4900 loss=tensor(0.007, grad_fn=<NllLossBackward0>)\n",
            "step=5000 loss=tensor(0.012, grad_fn=<NllLossBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# generation\n",
        "# dumb way: 1 token at a time. get logits, do softmax, sample token. feed in full generated thing back\n",
        "\n",
        "# kv cache way: attention takes in a kv cache which are pre-computed k and v values for previous tokens\n",
        "\n"
      ],
      "metadata": {
        "id": "0FbHrEmhMayx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Simple generation"
      ],
      "metadata": {
        "id": "TYAurIkmqA74"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def softmax_with_temp_last_dim(logits: torch.Tensor, temp: float=1.0, eps=1e-6):\n",
        "  # softmax across the innermost (last) dimension\n",
        "  # compute max\n",
        "  # subtract max\n",
        "  # numerator = exponentiate with temperature\n",
        "  # denominator = sum of numerator\n",
        "  #import pdb; pdb.set_trace()\n",
        "  # logits: (..., vocab_size)\n",
        "  maxval = torch.amax(logits, dim=-1).unsqueeze(-1)\n",
        "  max_subtracted_logits = logits - maxval\n",
        "  scaled_max_subtracted_logits = max_subtracted_logits/torch.Tensor([temp+eps])\n",
        "  numerator = torch.exp(scaled_max_subtracted_logits)\n",
        "  denominator = torch.sum(numerator, dim=-1).unsqueeze(-1)\n",
        "  return numerator/(denominator + eps)\n",
        "\n",
        "\n",
        "def retain_top_p(probs: torch.Tensor, p: float):\n",
        "  # retain only probabilities with mass = top_p\n",
        "  assert len(probs.shape) == 1 # batch later\n",
        "  sorted_probs = torch.sort(probs, descending=True)\n",
        "  sorted_probs_values = sorted_probs.values\n",
        "  sorted_probs_indices = sorted_probs.indices\n",
        "  cumsum_probs = list(torch.cumsum(sorted_probs_values, dim=0))\n",
        "  for i, prob in enumerate(cumsum_probs):  # super ugly, need vectorized version\n",
        "    if prob > p:\n",
        "      break\n",
        "  not_useful_indices = sorted_probs_indices[(i+1):]\n",
        "  if len(not_useful_indices) > 0:\n",
        "    probs[not_useful_indices] = 0 # TODO check here\n",
        "  return probs\n",
        "\n",
        "\n",
        "\n",
        "def sample(logits: torch.Tensor, top_p: Optional[float]=None, temp: float = 1.0):\n",
        "  # logits: (vocab_size,)\n",
        "  probs = softmax_with_temp_last_dim(logits=logits, temp=temp)\n",
        "  if top_p:\n",
        "    probs = retain_top_p(probs=probs, p=top_p)\n",
        "  sampled = np.random.multinomial(n=1, pvals=probs)\n",
        "  return int(np.argwhere(sampled)[0][0])\n",
        "\n",
        "\n",
        "def generate(m: torch.nn.Module, input_tokens: torch.Tensor, toks_to_generate: int):\n",
        "  # for each token, run forward\n",
        "    # run forward get logits\n",
        "    # convert logits to probabilities\n",
        "    # sample from probabilities\n",
        "    # update input tokens\n",
        "  pass"
      ],
      "metadata": {
        "id": "rtQTtV_sqCSL"
      },
      "execution_count": 113,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Simple tests"
      ],
      "metadata": {
        "id": "PVQertKT70yf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "logits = torch.tensor([5,1,1,5,1])\n",
        "sample(logits, temp=0)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-shFNf4v7z5A",
        "outputId": "e981d10d-cc98-414b-bfb8-96999ae8946b"
      },
      "execution_count": 120,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "3"
            ]
          },
          "metadata": {},
          "execution_count": 120
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Sample"
      ],
      "metadata": {
        "id": "CVPCOS8kEa2R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "probs"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YDpViMId8pqe",
        "outputId": "96410a6e-67b4-40cd-8b34-908d8c9fe63e"
      },
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([0.487, 0.009, 0.009, 0.487, 0.009])"
            ]
          },
          "metadata": {},
          "execution_count": 62
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "int(np.argwhere(np.random.multinomial(n=1, pvals=probs))[0][0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sI8RXUbQBFTY",
        "outputId": "ce77aa01-90aa-4dab-d7d4-09b077f10981"
      },
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "3"
            ]
          },
          "metadata": {},
          "execution_count": 57
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "KQDNdkfOCUfy"
      },
      "execution_count": 39,
      "outputs": []
    }
  ]
}