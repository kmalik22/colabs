{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNK+Jdk+UQXOJOKJ7W0SwJG",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kmalik22/colabs/blob/main/transformer_training_pytorch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 298,
      "metadata": {
        "id": "xKr0v3brEOJW"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import torch.nn.functional as F\n",
        "import itertools\n",
        "from typing import Optional, Tuple, List\n",
        "from torch.utils.data import IterableDataset, DataLoader\n",
        "torch.set_printoptions(linewidth=250, precision=3)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "a = torch.arange(24).reshape(2,3,4)\n",
        "b = torch.arange(100,124).reshape(2,3,4)\n",
        "print(a)\n",
        "print(b)\n",
        "\n",
        "torch.cat([a, b], dim=1) #expected shape: 2,6,4. 0-11. then 100-111"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kLl80eCaTK29",
        "outputId": "cf15815a-eca7-471a-f30f-51c78ae1c1b2"
      },
      "execution_count": 398,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[[ 0,  1,  2,  3],\n",
            "         [ 4,  5,  6,  7],\n",
            "         [ 8,  9, 10, 11]],\n",
            "\n",
            "        [[12, 13, 14, 15],\n",
            "         [16, 17, 18, 19],\n",
            "         [20, 21, 22, 23]]])\n",
            "tensor([[[100, 101, 102, 103],\n",
            "         [104, 105, 106, 107],\n",
            "         [108, 109, 110, 111]],\n",
            "\n",
            "        [[112, 113, 114, 115],\n",
            "         [116, 117, 118, 119],\n",
            "         [120, 121, 122, 123]]])\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[[  0,   1,   2,   3],\n",
              "         [  4,   5,   6,   7],\n",
              "         [  8,   9,  10,  11],\n",
              "         [100, 101, 102, 103],\n",
              "         [104, 105, 106, 107],\n",
              "         [108, 109, 110, 111]],\n",
              "\n",
              "        [[ 12,  13,  14,  15],\n",
              "         [ 16,  17,  18,  19],\n",
              "         [ 20,  21,  22,  23],\n",
              "         [112, 113, 114, 115],\n",
              "         [116, 117, 118, 119],\n",
              "         [120, 121, 122, 123]]])"
            ]
          },
          "metadata": {},
          "execution_count": 398
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Modules"
      ],
      "metadata": {
        "id": "FkrwnUY_aTuI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MLP(torch.nn.Module):\n",
        "  def __init__(self, d_model, d_ffn):\n",
        "    super().__init__()\n",
        "    self.d_model = d_model\n",
        "    self.d_ffn = d_ffn\n",
        "    self.fc1 = torch.nn.Linear(in_features=d_model, out_features=d_ffn, bias=False)\n",
        "    torch.nn.init.trunc_normal_(self.fc1.weight, mean=0, std=1/d_model**0.5)\n",
        "    self.fc2 = torch.nn.Linear(in_features=d_ffn, out_features=d_model, bias=False)\n",
        "    torch.nn.init.trunc_normal_(self.fc2.weight, mean=0, std=1/d_ffn**0.5, a=-3, b=3)\n",
        "\n",
        "  def forward(self, x: torch.Tensor):\n",
        "    fc1_out = self.fc1(x)\n",
        "    relu_out = torch.nn.functional.relu(fc1_out)\n",
        "    fc2_out = self.fc2(relu_out)\n",
        "    return fc2_out\n",
        "\n",
        "class MultiHeadAttention(torch.nn.Module):\n",
        "  def __init__(self, d_model, num_heads):\n",
        "    super().__init__()\n",
        "    self.d_model = d_model\n",
        "    self.num_heads = num_heads\n",
        "    self.head_dim = d_model//num_heads\n",
        "    self.wqkv = torch.nn.Linear(d_model, int(3*d_model), bias=False)\n",
        "    torch.nn.init.trunc_normal_(self.wqkv.weight, mean=0, std=1/d_model**0.5, a=-3, b=3)\n",
        "    self.wo = torch.nn.Linear(d_model, d_model)\n",
        "    torch.nn.init.trunc_normal_(self.wo.weight, mean=0, std=1/d_model**0.5, a=-3, b=3)\n",
        "    # TODO: add rope\n",
        "\n",
        "  def scaled_dot_product_attn(self, q, k, v):\n",
        "    # q = (bsz, num_heads, gen_seqlen, head_dim)\n",
        "    # ctx_seqle+gen_seqlen = total_seqlen\n",
        "    # k, v = (bsz, num_heads, total_seqlen, head_dim)\n",
        "    # qk.t()/sqrt(head_dim)\n",
        "    # causal mask\n",
        "    # softmax\n",
        "    # @ v\n",
        "    gen_seqlen = q.shape[2]\n",
        "    total_seqlen = k.shape[2]\n",
        "    assert gen_seqlen <= total_seqlen\n",
        "    attn_wts = q @ k.transpose(2,3)  #(bsz,num_h,gen_seqlen,head_dim) (bsz,num_h,head_dim,total_seqlen) -> (bsz,num_h,gen_seqlen,total_seqlen)\n",
        "    # create mask, do torch.where. 1=use attn wts, else use -inf\n",
        "    mask = torch.tril(torch.ones(total_seqlen,total_seqlen)).to(torch.bool)\n",
        "    mask = mask[(total_seqlen - gen_seqlen):,:]\n",
        "    masked_attn_wts = torch.where(mask, attn_wts, float('-inf')) # (bsz,num_heads,gen_seqlen,total_seqlen)\n",
        "\n",
        "    # softmax TODO check once\n",
        "    softmax_wts = torch.nn.functional.softmax(masked_attn_wts,dim=-1)\n",
        "    # (bsz,num_heads,gen_seqlen,total_seqlen) @ (bsz,num_heads,total_seqlen,head_dim) -> (bsz,num_heads,gen_seqlen,head_dim)\n",
        "    #print(f\"{softmax_wts.shape=}  {v.shape=}\")\n",
        "    return softmax_wts @ v\n",
        "\n",
        "\n",
        "  def forward(self, x:torch.Tensor, kv_cache: Optional[Tuple[torch.Tensor, torch.Tensor]]=None) -> Tuple[torch.Tensor, Tuple[torch.Tensor, torch.Tensor]]:\n",
        "    #x = (bsz, gen_seqlen, d_model)\n",
        "    bsz, gen_seqlen, d_model = x.shape\n",
        "    assert d_model == self.d_model\n",
        "    if kv_cache is not None:\n",
        "      assert kv_cache[0].shape[0] == bsz\n",
        "      assert kv_cache[0].shape[3] == self.head_dim, f\"{kv_cache[0].shape[3]=} {self.head_dim=}\"\n",
        "      assert kv_cache[0].shape[1] == self.num_heads, f\"{kv_cache[0].shape[2]=} {self.num_heads=}\"\n",
        "      assert kv_cache[0].shape == kv_cache[1].shape\n",
        "      ctx_seqlen = kv_cache[0].shape[1]\n",
        "    else:\n",
        "      ctx_seqlen = gen_seqlen\n",
        "    wqkv_out = self.wqkv(x) # (bsz, gen_seqlen, 3*d_model)\n",
        "    #print(f\"wqkv_out:{wqkv_out[0,0,:].norm()}\")\n",
        "    # rearrange so seqlen,head_dim are the last two dims\n",
        "    wqkv_out = wqkv_out.reshape(bsz, gen_seqlen, 3, self.num_heads, self.head_dim)\n",
        "    #(bsz,seqlen,3,num_heads,head_dim) -> (bsz,3,num_heads,seqlen,head_dim)\n",
        "    #  0    1    2    3        4\n",
        "    wqkv_out = wqkv_out.permute(0, 2, 3, 1, 4)  # (bsz, 3, num_heads, seqlen, head_dim)\n",
        "    q = wqkv_out[:,0,:,:,:] # (bsz, num_heads, gen_seqlen, head_dim)\n",
        "    k = wqkv_out[:,1,:,:,:] # (bsz, num_heads, gen_seqlen, head_dim)\n",
        "    v = wqkv_out[:,2,:,:,:] # (bsz, num_heads, gen_seqlen, head_dim)\n",
        "    if kv_cache is not None:\n",
        "      #print(f\"{k.shape=} {kv_cache[0].shape=} final_k.shape={torch.cat([kv_cache[0], k], dim=2).shape}\")\n",
        "      k = torch.cat([kv_cache[0], k], dim=2)\n",
        "      v = torch.cat([kv_cache[1], v], dim=2)\n",
        "\n",
        "    #print(f\"q first token: {q[0,:,0,:].norm()}  q second token: {q[0,:,1,:].norm()}\")\n",
        "    #print(f\"k first token: {k[0,:,0,:].norm()}  k second token: {k[0,:,1,:].norm()}\")\n",
        "    #print(f\"v first token: {v[0,:,0,:].norm()}  v second token: {v[0,:,1,:].norm()}\")\n",
        "\n",
        "    # now k and v are (bsx, num_hads, (ctx_seqlen+gen_seqlen), head_dim)\n",
        "    # TODO: add rope to q and k\n",
        "    #print(f\"q:{q[0,:,0,:].norm()}, k:{k[0,:,0,:].norm()}, v:{v[0,:,0,:].norm()}\")\n",
        "    self_attn_out = self.scaled_dot_product_attn(q, k, v) #(bsz, num_heads, gen_seqlen, head_dim)\n",
        "    #print(f\"self_attn_out first token:{self_attn_out[0,:,0,:].norm()} self_attn_out second token:{self_attn_out[0,:,1,:].norm()}\")\n",
        "    # transpose\n",
        "    self_attn_out = self_attn_out.transpose(1,2) # (bsz,gen_seqlen,num_heads,head_dim)\n",
        "    # concat the last dim\n",
        "    self_attn_out = self_attn_out.reshape(bsz,gen_seqlen,-1)\n",
        "    # wo\n",
        "    self_attn_out = self_attn_out.reshape(bsz, gen_seqlen, d_model)\n",
        "    wo_out = self.wo(self_attn_out)\n",
        "    return wo_out, (k, v)\n",
        "\n",
        "\n",
        "class TransformerBlock(torch.nn.Module):\n",
        "  def __init__(self, d_model, d_ffn, num_heads, max_seqlen):\n",
        "    super().__init__()\n",
        "    self.mlp_norm = torch.nn.RMSNorm(normalized_shape=d_model)\n",
        "    self.mlp = MLP(d_model=d_model, d_ffn=d_ffn)\n",
        "    self.attn_norm = torch.nn.RMSNorm(normalized_shape=d_model)\n",
        "    self.attn = MultiHeadAttention(d_model=d_model,num_heads=num_heads)\n",
        "\n",
        "\n",
        "  def forward(self, x:torch.Tensor) -> Tuple[torch.Tensor, Tuple[torch.Tensor, torch.Tensor]]:\n",
        "    # norm --> attn, add back to original --> norm --> mlp, add back\n",
        "    attn_norm_out = self.attn_norm(x)\n",
        "    #print(f\"attn_norm_out: {attn_norm_out[0,0,:]}\")\n",
        "    attn_out, (k, v) = self.attn(attn_norm_out)\n",
        "    #print(f\"attn_out: {attn_out[0,0,:]}\")\n",
        "    attn_out_post_resid = x + attn_out\n",
        "    mlp_norm_out = self.mlp_norm(attn_out_post_resid)\n",
        "    mlp_out = self.mlp(mlp_norm_out)\n",
        "    return mlp_out + attn_out_post_resid, (k, v)\n",
        "\n",
        "class Transformer(torch.nn.Module):\n",
        "  def __init__(self, d_model, d_ffn, vocab_size, num_layers, max_seqlen, num_heads, simple_pos_embed=False):\n",
        "    super().__init__()\n",
        "    self.max_seqlen = max_seqlen\n",
        "    self.embedding = torch.nn.Embedding(num_embeddings=vocab_size, embedding_dim=d_model)\n",
        "    torch.nn.init.trunc_normal_(self.embedding.weight, mean=0, std=1/d_model**0.5, a=-3, b=-3)\n",
        "\n",
        "    self.simple_pos_embed = simple_pos_embed\n",
        "    if simple_pos_embed:\n",
        "      self.pos_embedding = torch.nn.Embedding(num_embeddings=max_seqlen, embedding_dim=d_model)\n",
        "\n",
        "    self.output_layer = torch.nn.Linear(in_features=d_model, out_features=vocab_size)\n",
        "    torch.nn.init.trunc_normal_(self.output_layer.weight, mean=0, std=1/d_model**0.5, a=-3, b=3)\n",
        "    self.layers = torch.nn.ModuleList(\n",
        "        [TransformerBlock(d_model=d_model, d_ffn=d_ffn, num_heads=num_heads, max_seqlen=max_seqlen) for _ in range(num_layers)]\n",
        "    )\n",
        "    # TODO: do proper init\n",
        "    # add attention\n",
        "    # add layer norms\n",
        "\n",
        "  def forward(self, tokens: torch.Tensor) -> Tuple[torch.Tensor, List[Tuple[torch.Tensor, torch.Tensor]]]:\n",
        "    # tokens: (bsz,seqlen,d_model)\n",
        "    kv_cache: List[Tuple[torch.Tensor, torch.Tensor]] = []\n",
        "    seqlen = tokens.shape[1]\n",
        "    curr_out = self.embedding(tokens)\n",
        "    if self.simple_pos_embed:\n",
        "      curr_out = curr_out + self.pos_embedding(torch.arange(seqlen))\n",
        "    for l in self.layers:\n",
        "      layer_out, (k, v) = l(curr_out)\n",
        "      curr_out = curr_out + layer_out\n",
        "      kv_cache.append((k,v))\n",
        "    logits = self.output_layer(curr_out)  # bsz,seqlen,vocab_size\n",
        "    return logits, kv_cache\n"
      ],
      "metadata": {
        "id": "KgGPaK6cERnA"
      },
      "execution_count": 417,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Test code: transformer block is causal"
      ],
      "metadata": {
        "id": "J3MNY6h3aQUh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with torch.no_grad():\n",
        "  tblock_test = TransformerBlock(d_model=D_MODEL, d_ffn=D_FFN, num_heads=NUM_HEADS, max_seqlen=SEQLEN)\n",
        "  input_act = torch.randn(BSZ, SEQLEN, D_MODEL)\n",
        "  tblock_out, _ = tblock_test(input_act)\n",
        "  assert tblock_out.shape == (BSZ, SEQLEN, D_MODEL)\n",
        "\n",
        "  input_act_trunc = input_act[:,:2,:]\n",
        "  tblock_out2, _ = tblock_test(input_act_trunc)\n",
        "\n",
        "  assert torch.all(torch.isclose(tblock_out[0,0,:], tblock_out2[0,0,:]))\n",
        "  print(\"Logits match\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "plQSbdRDZRDC",
        "outputId": "d97ce192-afc9-4380-c741-9e88b59bc471"
      },
      "execution_count": 418,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Logits match\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Test: MHA with KV cache works same as MHA"
      ],
      "metadata": {
        "id": "yOiZ1MpaGv1M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# step 1: just return KV values\n",
        "mha_test = MultiHeadAttention(d_model=D_MODEL, num_heads=NUM_HEADS)\n",
        "input_act = torch.randn(BSZ, SEQLEN, D_MODEL)\n",
        "mha_out, kv_cache = mha_test(input_act)\n",
        "#print(f\"input second token:{input_act[0,1,:].norm()} second token: {mha_out[0,1,:].norm()}\")\n",
        "\n",
        "\n",
        "# step 2: eat first token in input. feed this token in as kv-cache instead\n",
        "input_act2 = input_act[:,1:,:]\n",
        "# kv_cache: (bsz,num_heads,seqlen,head_dim)\n",
        "kv_first_token = kv_cache[0][:,:,:1,:], kv_cache[1][:,:,:1,:]\n",
        "mha_out2, kv_cache2 = mha_test(input_act2, kv_cache=kv_first_token)\n",
        "#print(f\"input second token:{input_act2[0,0,:].norm()} second token: {mha_out2[0,0,:].norm()}\")\n",
        "\n",
        "assert torch.all(torch.isclose(mha_out[0,1,:], mha_out2[0,0,:]))\n",
        "print(\"Output token logits match\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UmyRoh6QGzal",
        "outputId": "0082f362-0b53-4035-a386-1b6e509a5eea"
      },
      "execution_count": 419,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output token logits match\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Dataloader"
      ],
      "metadata": {
        "id": "tuQgoyYXaPL2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "EOS_TOKEN=-1\n",
        "\n",
        "class DigitDataset(IterableDataset):\n",
        "  def __init__(self, num_digits=10):\n",
        "    self.num_digits = 10\n",
        "\n",
        "  def __iter__(self):\n",
        "    while True:\n",
        "      for i in range(self.num_digits):\n",
        "        yield i\n",
        "\n",
        "\n",
        "def collate_batch_digits(batch, seqlen, bsz):\n",
        "  x = torch.tensor(batch, dtype=torch.long).view(bsz, seqlen)\n",
        "  y = torch.roll(x, shifts=-1, dims=(1,))\n",
        "  y[:,-1] = EOS_TOKEN\n",
        "  return x,y\n",
        "\n",
        "# dataset that spits out 8 unsorted digits, then sorts them all\n",
        "\n",
        "class SortedDigitsDataset(IterableDataset):\n",
        "  # outputs a full batch\n",
        "  def __init__(self, num_digits=10, max_seqlen=8, eos_token=EOS_TOKEN):\n",
        "    self.num_digits = 10\n",
        "    self.max_seqlen = max_seqlen\n",
        "    self.eos_token = eos_token\n",
        "\n",
        "  def __iter__(self):\n",
        "    while True:\n",
        "      half = int(self.max_seqlen//2)\n",
        "      half_batch = torch.randint(low=0,high=(self.num_digits-1), size=(half,))\n",
        "      sorted = torch.sort(half_batch).values\n",
        "      batch = torch.cat([half_batch, sorted])\n",
        "      targets = torch.roll(batch, shifts=-1)\n",
        "      targets[:int(half)-1] = self.eos_token\n",
        "      targets[-1] = self.eos_token\n",
        "      #print(f\"batch:{batch}\\n targets:{targets}\\n\")\n",
        "      yield (batch, targets)\n"
      ],
      "metadata": {
        "id": "OZPMJ9VHG2na"
      },
      "execution_count": 420,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Dataloader test"
      ],
      "metadata": {
        "id": "fhA1W-c9fuxK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sorted_digits_dataset = SortedDigitsDataset(max_seqlen=SEQLEN)\n",
        "sorted_dataloader = DataLoader(dataset=sorted_digits_dataset, batch_size=BSZ, shuffle=False)\n",
        "\n",
        "an_iter = iter(sorted_digits_dataset)\n",
        "while True:\n",
        "  print(next(an_iter))\n",
        "  break"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KByqwSM1fxyz",
        "outputId": "c5ed13b5-af2d-4b3c-fda3-02cc5e8caab3"
      },
      "execution_count": 421,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(tensor([0, 0, 0, 7, 7, 1, 7, 6, 0, 0, 0, 1, 6, 7, 7, 7]), tensor([-1, -1, -1, -1, -1, -1, -1,  0,  0,  0,  1,  6,  7,  7,  7, -1]))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Constants"
      ],
      "metadata": {
        "id": "K6UBpVpR0w1T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "D_MODEL=16\n",
        "D_FFN=64\n",
        "NUM_LAYERS=2\n",
        "VOCAB_SIZE=16\n",
        "SEQLEN=16\n",
        "BSZ=32\n",
        "NUM_HEADS=4\n",
        "HEAD_DIM=D_MODEL//NUM_HEADS"
      ],
      "metadata": {
        "id": "iwsF3nnZ0wMK"
      },
      "execution_count": 422,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Dataloader"
      ],
      "metadata": {
        "id": "NR6aFw_S0z0P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sorted_digits_dataset = SortedDigitsDataset(max_seqlen=SEQLEN)\n",
        "sorted_dataloader = DataLoader(dataset=sorted_digits_dataset, batch_size=BSZ, shuffle=False)\n",
        "dl_iter = iter(sorted_dataloader)"
      ],
      "metadata": {
        "id": "isnsjgkh008a"
      },
      "execution_count": 423,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Trainer"
      ],
      "metadata": {
        "id": "Uxlg3n9kaR0O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "STEPS=5001\n",
        "\n",
        "torch.manual_seed(100)\n",
        "model = Transformer(d_model=D_MODEL, d_ffn=D_FFN, vocab_size=VOCAB_SIZE, num_layers=NUM_LAYERS, num_heads=NUM_HEADS, max_seqlen=SEQLEN, simple_pos_embed=True)\n",
        "optimizer = torch.optim.AdamW(params=model.parameters(), lr=0.001, weight_decay=0)\n",
        "\n",
        "#digits = DigitDataset()\n",
        "#digit_dataloader = DataLoader(dataset=digits, batch_size=SEQLEN*BSZ, shuffle=False, collate_fn=lambda b:collate_batch_digits(batch=b, seqlen=SEQLEN, bsz=BSZ))\n",
        "#dl_iter = iter(digit_dataloader)\n",
        "\n",
        "\n",
        "for step in range(STEPS):\n",
        "  optimizer.zero_grad()\n",
        "  tokens, targets = next(dl_iter) #targets = (bsz, seqlen)   tokens=(bsz, seqlen)\n",
        "  logits, _ = model(tokens)  # (bsz, seqlen, vocab_size)\n",
        "  vocab_size = logits.shape[-1]\n",
        "  loss = torch.nn.functional.cross_entropy(input=logits.view(-1, vocab_size), target=targets.view(-1), ignore_index=EOS_TOKEN)\n",
        "  if step %100 == 0:\n",
        "    print(f\"{step=} {loss=}\")\n",
        "    #print(f\"tokens:{tokens[0,:]} \\n targets:{targets[0,:]} \\n\\n\")\n",
        "    #print(f\"tokens:{tokens[0,:]} \\n targets:{targets[0,:]} \\n probs:\\n{logits[0,:,:]}\\n\")\n",
        "  loss.backward()\n",
        "  optimizer.step()\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ms1MEO7EGIDj",
        "outputId": "9eca0b66-0cd8-41a1-f490-126be71d4f41"
      },
      "execution_count": 424,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step=0 loss=tensor(26.213, grad_fn=<NllLossBackward0>)\n",
            "step=100 loss=tensor(2.689, grad_fn=<NllLossBackward0>)\n",
            "step=200 loss=tensor(1.623, grad_fn=<NllLossBackward0>)\n",
            "step=300 loss=tensor(1.298, grad_fn=<NllLossBackward0>)\n",
            "step=400 loss=tensor(1.338, grad_fn=<NllLossBackward0>)\n",
            "step=500 loss=tensor(1.159, grad_fn=<NllLossBackward0>)\n",
            "step=600 loss=tensor(1.160, grad_fn=<NllLossBackward0>)\n",
            "step=700 loss=tensor(1.182, grad_fn=<NllLossBackward0>)\n",
            "step=800 loss=tensor(1.132, grad_fn=<NllLossBackward0>)\n",
            "step=900 loss=tensor(1.042, grad_fn=<NllLossBackward0>)\n",
            "step=1000 loss=tensor(1.032, grad_fn=<NllLossBackward0>)\n",
            "step=1100 loss=tensor(1.007, grad_fn=<NllLossBackward0>)\n",
            "step=1200 loss=tensor(0.990, grad_fn=<NllLossBackward0>)\n",
            "step=1300 loss=tensor(0.941, grad_fn=<NllLossBackward0>)\n",
            "step=1400 loss=tensor(0.948, grad_fn=<NllLossBackward0>)\n",
            "step=1500 loss=tensor(0.887, grad_fn=<NllLossBackward0>)\n",
            "step=1600 loss=tensor(0.813, grad_fn=<NllLossBackward0>)\n",
            "step=1700 loss=tensor(0.806, grad_fn=<NllLossBackward0>)\n",
            "step=1800 loss=tensor(0.724, grad_fn=<NllLossBackward0>)\n",
            "step=1900 loss=tensor(0.688, grad_fn=<NllLossBackward0>)\n",
            "step=2000 loss=tensor(0.714, grad_fn=<NllLossBackward0>)\n",
            "step=2100 loss=tensor(0.667, grad_fn=<NllLossBackward0>)\n",
            "step=2200 loss=tensor(0.553, grad_fn=<NllLossBackward0>)\n",
            "step=2300 loss=tensor(0.502, grad_fn=<NllLossBackward0>)\n",
            "step=2400 loss=tensor(0.476, grad_fn=<NllLossBackward0>)\n",
            "step=2500 loss=tensor(0.462, grad_fn=<NllLossBackward0>)\n",
            "step=2600 loss=tensor(0.385, grad_fn=<NllLossBackward0>)\n",
            "step=2700 loss=tensor(0.355, grad_fn=<NllLossBackward0>)\n",
            "step=2800 loss=tensor(0.304, grad_fn=<NllLossBackward0>)\n",
            "step=2900 loss=tensor(0.312, grad_fn=<NllLossBackward0>)\n",
            "step=3000 loss=tensor(0.238, grad_fn=<NllLossBackward0>)\n",
            "step=3100 loss=tensor(0.233, grad_fn=<NllLossBackward0>)\n",
            "step=3200 loss=tensor(0.187, grad_fn=<NllLossBackward0>)\n",
            "step=3300 loss=tensor(0.172, grad_fn=<NllLossBackward0>)\n",
            "step=3400 loss=tensor(0.157, grad_fn=<NllLossBackward0>)\n",
            "step=3500 loss=tensor(0.121, grad_fn=<NllLossBackward0>)\n",
            "step=3600 loss=tensor(0.080, grad_fn=<NllLossBackward0>)\n",
            "step=3700 loss=tensor(0.107, grad_fn=<NllLossBackward0>)\n",
            "step=3800 loss=tensor(0.076, grad_fn=<NllLossBackward0>)\n",
            "step=3900 loss=tensor(0.065, grad_fn=<NllLossBackward0>)\n",
            "step=4000 loss=tensor(0.044, grad_fn=<NllLossBackward0>)\n",
            "step=4100 loss=tensor(0.026, grad_fn=<NllLossBackward0>)\n",
            "step=4200 loss=tensor(0.029, grad_fn=<NllLossBackward0>)\n",
            "step=4300 loss=tensor(0.042, grad_fn=<NllLossBackward0>)\n",
            "step=4400 loss=tensor(0.019, grad_fn=<NllLossBackward0>)\n",
            "step=4500 loss=tensor(0.036, grad_fn=<NllLossBackward0>)\n",
            "step=4600 loss=tensor(0.033, grad_fn=<NllLossBackward0>)\n",
            "step=4700 loss=tensor(0.013, grad_fn=<NllLossBackward0>)\n",
            "step=4800 loss=tensor(0.022, grad_fn=<NllLossBackward0>)\n",
            "step=4900 loss=tensor(0.011, grad_fn=<NllLossBackward0>)\n",
            "step=5000 loss=tensor(0.020, grad_fn=<NllLossBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# kv cache\n",
        "# mha takes in kv_cache. if present, k and v values come from cache, PLUS add k and v for the current token\n",
        "# wqkv operates on seqlen=1. q will one be 1\n"
      ],
      "metadata": {
        "id": "0FbHrEmhMayx"
      },
      "execution_count": 425,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Simple generation"
      ],
      "metadata": {
        "id": "TYAurIkmqA74"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def softmax_with_temp_last_dim(logits: torch.Tensor, temp: float=1.0, eps=1e-6):\n",
        "  # softmax across the innermost (last) dimension\n",
        "  # compute max\n",
        "  # subtract max\n",
        "  # numerator = exponentiate with temperature\n",
        "  # denominator = sum of numerator\n",
        "  #import pdb; pdb.set_trace()\n",
        "  # logits: (..., vocab_size)\n",
        "  maxval = torch.amax(logits, dim=-1).unsqueeze(-1)\n",
        "  max_subtracted_logits = logits - maxval\n",
        "  scaled_max_subtracted_logits = max_subtracted_logits/torch.Tensor([temp+eps])\n",
        "  numerator = torch.exp(scaled_max_subtracted_logits)\n",
        "  denominator = torch.sum(numerator, dim=-1).unsqueeze(-1)\n",
        "  return numerator/(denominator + eps)\n",
        "\n",
        "\n",
        "def retain_top_p(probs: torch.Tensor, p: float):\n",
        "  # retain only probabilities with mass = top_p\n",
        "  assert len(probs.shape) == 1 # batch later\n",
        "  sorted_probs = torch.sort(probs, descending=True)\n",
        "  sorted_probs_values = sorted_probs.values\n",
        "  sorted_probs_indices = sorted_probs.indices\n",
        "  cumsum_probs = list(torch.cumsum(sorted_probs_values, dim=0))\n",
        "  for i, prob in enumerate(cumsum_probs):  # super ugly, need vectorized version\n",
        "    if prob > p:\n",
        "      break\n",
        "  not_useful_indices = sorted_probs_indices[(i+1):]\n",
        "  if len(not_useful_indices) > 0:\n",
        "    probs[not_useful_indices] = 0 # TODO check here\n",
        "  return probs\n",
        "\n",
        "\n",
        "\n",
        "def sample(logits: torch.Tensor, top_p: Optional[float]=None, temp: float = 1.0):\n",
        "  # logits: (vocab_size,)\n",
        "  probs = softmax_with_temp_last_dim(logits=logits, temp=temp)\n",
        "  #print(probs)\n",
        "  if top_p:\n",
        "    probs = retain_top_p(probs=probs, p=top_p)\n",
        "  sampled = np.random.multinomial(n=1, pvals=probs)\n",
        "  return int(np.argwhere(sampled)[0][0])\n",
        "\n",
        "\n",
        "def generate(the_model: torch.nn.Module, input_tokens: torch.Tensor, toks_to_generate: int, temp=1.0):\n",
        "  # for each token, run forward\n",
        "    # run forward get logits\n",
        "    # convert logits to probabilities\n",
        "    # sample from probabilities\n",
        "    # update input tokens\n",
        "  generated_toks = []\n",
        "  for _ in range(toks_to_generate):\n",
        "    logits, _ = the_model(input_tokens)\n",
        "    #print(f\"logits.shape:{logits.shape}\")\n",
        "    #print(f\"full_logits:{logits}\")\n",
        "    logits = logits.squeeze(0)[-1,:] #unsqueeze to remove batch dim. -1 for last token\n",
        "    #print(f\"logits:{logits}\")\n",
        "    sampled_token = sample(logits, temp=temp)\n",
        "    generated_toks.append(sampled_token)\n",
        "    #print(input_tokens.shape)\n",
        "    #print(torch.tensor([sampled_token]).unsqueeze(0).shape)\n",
        "    input_tokens = torch.cat([input_tokens.squeeze(0), torch.tensor([sampled_token])]).unsqueeze(0)\n",
        "  return generated_toks"
      ],
      "metadata": {
        "id": "rtQTtV_sqCSL"
      },
      "execution_count": 426,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Simple tests"
      ],
      "metadata": {
        "id": "PVQertKT70yf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "input, targets = next(dl_iter) #get the first entry only\n",
        "input = input[:1,:]\n",
        "targets = targets[:1,:]\n",
        "print(input)\n",
        "print(targets)\n",
        "trunc_input = input[:1,:int(SEQLEN//2)]\n",
        "print(trunc_input)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nlqqXCBI51Wy",
        "outputId": "6770515a-59bb-4b36-9077-0ebbbad6f0dd"
      },
      "execution_count": 427,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[4, 0, 8, 0, 8, 2, 4, 4, 0, 0, 2, 4, 4, 4, 8, 8]])\n",
            "tensor([[-1, -1, -1, -1, -1, -1, -1,  0,  0,  2,  4,  4,  4,  8,  8, -1]])\n",
            "tensor([[4, 0, 8, 0, 8, 2, 4, 4]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "logits_raw, _ = model(input)\n",
        "print(f\"raw_logits:{logits_raw}\")\n",
        "print(f\"logits for last:{logits_raw.squeeze(0)[7,:]}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tLKWEyqo6ydq",
        "outputId": "f1717af4-43ef-47f9-95b6-88eea5b96af6"
      },
      "execution_count": 428,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "raw_logits:tensor([[[ -8.898, -15.959,  -1.909,  16.486,  30.774,  -1.500,  -7.926, -21.109, -37.579,   0.783, -13.763,  -6.702,   0.223, -23.742, -11.606, -13.094],\n",
            "         [ 62.565,  12.333, -24.260, -21.372, -27.307, -25.315, -41.662, -17.038, -15.714,  12.552,  15.028,   8.288,   2.274,   5.355, -12.679,  23.037],\n",
            "         [ 34.121,  20.171, -16.655,  -9.214, -10.213,  -6.413, -34.274, -19.019,   5.486,   8.353,  17.150,   6.676,  -1.785,   8.697,  -6.161,  10.700],\n",
            "         [ 51.282,  24.071, -26.236, -13.121, -26.347, -27.381, -35.450, -21.988,  -5.609,   5.431,  15.267,   3.178,  -4.136,   4.262, -19.274,  10.499],\n",
            "         [-19.772, -11.509, -15.624,   1.613,  20.270,  28.447,   2.317, -12.397,   1.911,  -5.104,  -4.801,  -3.112, -15.362,  -9.443,  -7.918, -10.321],\n",
            "         [ 34.789,   5.031,  -1.144, -14.085,  -1.573,  -8.980, -31.694, -24.887, -25.228,  14.106,   6.898,  15.158,   7.803,   7.606,  -1.116,  14.085],\n",
            "         [ 35.788,   1.865, -19.196, -20.822,   1.793, -10.520, -29.875, -25.470, -26.295,  14.033,   8.463,   6.745,   3.438,   1.121,  -9.078,  11.837],\n",
            "         [ 45.176,  17.316, -14.345, -17.650,  -6.971, -18.543, -37.544, -25.708, -16.300,  10.331,   6.813,   5.641,  -1.899,   1.703, -10.768,   9.335],\n",
            "         [ 28.321,  19.068,   8.976,  -1.665,  -2.709, -20.484, -28.670, -21.464, -19.639, -11.723, -12.108, -14.325,  -8.749, -15.223,   0.983, -12.002],\n",
            "         [ 10.194,  14.805,  22.624,   7.968,   7.489, -17.737, -18.626, -20.144, -23.182, -15.045, -21.978, -14.120,  -9.057, -19.857,   3.387, -21.308],\n",
            "         [-22.815, -20.514,  18.047,  26.287,  39.830,   5.398,  -6.316, -13.824, -33.944, -11.278, -29.004, -15.739,  -8.007, -34.089,   1.707, -22.515],\n",
            "         [-27.775, -32.997, -17.586,   5.881,  40.019,  18.730,   8.505,  -2.280, -18.140,  -8.506, -19.252, -20.777, -15.334, -33.780,  -8.799, -20.193],\n",
            "         [-21.095, -32.935, -26.660,  -7.896,  26.542,  16.417,  14.743,  12.020,  -0.244,  -0.864,  -6.438, -12.158,  -8.975, -19.722,  -8.500,  -7.699],\n",
            "         [ -6.089, -19.396, -35.311, -18.440,   5.911,   8.073,  13.969,  29.280,  33.499,   2.673,   8.327, -11.695,  -6.070,  -7.196,  -7.604,   2.445],\n",
            "         [  9.833,   4.870, -36.581, -23.101, -12.419,   3.563,  -5.541,  41.594,  81.574,   3.717,  28.689,  -5.634,  -3.743,   8.567,   1.264,  11.026],\n",
            "         [-11.604, -21.993, -39.555, -26.325,  -0.285,  34.178,  17.533,  17.147,  34.509,   4.936,  19.378,   6.073,  -9.026,  10.761,  -6.014,   8.960]]], grad_fn=<ViewBackward0>)\n",
            "logits for last:tensor([ 45.176,  17.316, -14.345, -17.650,  -6.971, -18.543, -37.544, -25.708, -16.300,  10.331,   6.813,   5.641,  -1.899,   1.703, -10.768,   9.335], grad_fn=<SliceBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# test for sampling functions\n",
        "#logits = torch.tensor([5,1,1,5,1])\n",
        "#sample(logits, temp=0)\n",
        "\n",
        "with torch.no_grad():\n",
        "  # test for full generation\n",
        "  input = next(dl_iter)[0][0,:].unsqueeze(0) #get the first entry only\n",
        "  trunc_input = input[:1,:int(SEQLEN//2)]\n",
        "  generated = generate(the_model=model, input_tokens=trunc_input, toks_to_generate=8, temp=0.01)\n",
        "  print(f\"input:{input}, expected_op:{input[:1, int(SEQLEN//2):]},  generated:{generated}\")\n",
        "  assert torch.all(torch.isclose(input[:1, int(SEQLEN//2):], torch.tensor(generated)))\n",
        "  print(f\"Output is sorted\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-shFNf4v7z5A",
        "outputId": "ced97e9c-34f5-4f04-fd0a-005551b07bb1"
      },
      "execution_count": 429,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "input:tensor([[6, 6, 4, 1, 8, 0, 3, 5, 0, 1, 3, 4, 5, 6, 6, 8]]), expected_op:tensor([[0, 1, 3, 4, 5, 6, 6, 8]]),  generated:[0, 1, 3, 4, 5, 6, 6, 8]\n",
            "Output is sorted\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Sample"
      ],
      "metadata": {
        "id": "CVPCOS8kEa2R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YDpViMId8pqe",
        "outputId": "f7e02f6d-40ee-4bb2-f483-d1d90fc43840"
      },
      "execution_count": 318,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Transformer(\n",
              "  (embedding): Embedding(16, 16)\n",
              "  (pos_embedding): Embedding(16, 16)\n",
              "  (output_layer): Linear(in_features=16, out_features=16, bias=True)\n",
              "  (layers): ModuleList(\n",
              "    (0-1): 2 x TransformerBlock(\n",
              "      (mlp_norm): RMSNorm((16,), eps=None, elementwise_affine=True)\n",
              "      (mlp): MLP(\n",
              "        (fc1): Linear(in_features=16, out_features=64, bias=False)\n",
              "        (fc2): Linear(in_features=64, out_features=16, bias=False)\n",
              "      )\n",
              "      (attn_norm): RMSNorm((16,), eps=None, elementwise_affine=True)\n",
              "      (attn): MultiHeadAttention(\n",
              "        (wqkv): Linear(in_features=16, out_features=48, bias=False)\n",
              "        (wo): Linear(in_features=16, out_features=16, bias=True)\n",
              "      )\n",
              "    )\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 318
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "int(np.argwhere(np.random.multinomial(n=1, pvals=probs))[0][0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sI8RXUbQBFTY",
        "outputId": "ce77aa01-90aa-4dab-d7d4-09b077f10981"
      },
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "3"
            ]
          },
          "metadata": {},
          "execution_count": 57
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KQDNdkfOCUfy",
        "outputId": "5a91deb7-a731-4d92-d305-5f3560da060f"
      },
      "execution_count": 297,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Transformer(\n",
              "  (embedding): Embedding(16, 16)\n",
              "  (pos_embedding): Embedding(16, 16)\n",
              "  (output_layer): Linear(in_features=16, out_features=16, bias=True)\n",
              "  (layers): ModuleList(\n",
              "    (0-1): 2 x TransformerBlock(\n",
              "      (mlp_norm): RMSNorm((16,), eps=None, elementwise_affine=True)\n",
              "      (mlp): MLP(\n",
              "        (fc1): Linear(in_features=16, out_features=64, bias=False)\n",
              "        (fc2): Linear(in_features=64, out_features=16, bias=False)\n",
              "      )\n",
              "      (attn_norm): RMSNorm((16,), eps=None, elementwise_affine=True)\n",
              "      (attn): MultiHeadAttention(\n",
              "        (wqkv): Linear(in_features=16, out_features=48, bias=False)\n",
              "        (wo): Linear(in_features=16, out_features=16, bias=True)\n",
              "      )\n",
              "    )\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 297
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "aLwixnft04Bb"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}