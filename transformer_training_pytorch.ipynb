{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPGK6pXFJDPb2EeGNTSiLvr",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kmalik22/colabs/blob/main/transformer_training_pytorch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 125,
      "metadata": {
        "id": "xKr0v3brEOJW"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import torch.nn.functional as F\n",
        "import itertools\n",
        "from typing import Optional\n",
        "from torch.utils.data import IterableDataset, DataLoader\n",
        "torch.set_printoptions(linewidth=250, precision=3)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Modules"
      ],
      "metadata": {
        "id": "FkrwnUY_aTuI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MLP(torch.nn.Module):\n",
        "  def __init__(self, d_model, d_ffn):\n",
        "    super().__init__()\n",
        "    self.d_model = d_model\n",
        "    self.d_ffn = d_ffn\n",
        "    self.fc1 = torch.nn.Linear(in_features=d_model, out_features=d_ffn, bias=False)\n",
        "    torch.nn.init.trunc_normal_(self.fc1.weight, mean=0, std=1/d_model**0.5)\n",
        "    self.fc2 = torch.nn.Linear(in_features=d_ffn, out_features=d_model, bias=False)\n",
        "    torch.nn.init.trunc_normal_(self.fc2.weight, mean=0, std=1/d_ffn**0.5, a=-3, b=3)\n",
        "\n",
        "  def forward(self, x: torch.Tensor):\n",
        "    fc1_out = self.fc1(x)\n",
        "    relu_out = torch.nn.functional.relu(fc1_out)\n",
        "    fc2_out = self.fc2(relu_out)\n",
        "    return fc2_out\n",
        "\n",
        "class MultiHeadAttention(torch.nn.Module):\n",
        "  def __init__(self, d_model, num_heads):\n",
        "    super().__init__()\n",
        "    self.d_model = d_model\n",
        "    self.num_heads = num_heads\n",
        "    self.head_dim = d_model//num_heads\n",
        "    self.wqkv = torch.nn.Linear(d_model, int(3*d_model), bias=False)\n",
        "    torch.nn.init.trunc_normal_(self.wqkv.weight, mean=0, std=1/d_model**0.5, a=-3, b=3)\n",
        "    self.wo = torch.nn.Linear(d_model, d_model)\n",
        "    torch.nn.init.trunc_normal_(self.wo.weight, mean=0, std=1/d_model**0.5, a=-3, b=3)\n",
        "    # TODO: add rope\n",
        "\n",
        "  def scaled_dot_product_attn(self, q, k, v):\n",
        "    # q, k, v = (bsz, num_heads, seqlen, head_dim)\n",
        "    # qk.t()/sqrt(head_dim)\n",
        "    # causal mask\n",
        "    # softmax\n",
        "    # @ v\n",
        "    seqlen = q.shape[2]\n",
        "    attn_wts = q @ k.transpose(2,3)  #(bsz,num_h,seqlen,head_dim) (bsz,num_h,head_dim,seqlen) -> (bsz,num_h,seqlen,seqlen)\n",
        "    # create mask, do torch.where. 1=use attn wts, else use -inf\n",
        "    mask = torch.tril(torch.ones(seqlen,seqlen)).to(torch.bool)\n",
        "    masked_attn_wts = torch.where(mask, attn_wts, float('-inf')) # (bsz,num_heads,seqlen,seqlen)\n",
        "\n",
        "    # softmax TODO check once\n",
        "    softmax_wts = torch.nn.functional.softmax(masked_attn_wts,dim=-1)\n",
        "    # (bsz,num_heads,seqlen,seqlen) @ (bsz,num_heasds,seqlen,head_dim) -> (bsz,num_heads,seqlen,head_dim)\n",
        "    return softmax_wts @ v\n",
        "\n",
        "\n",
        "  def forward(self, x:torch.Tensor):\n",
        "    #x = (bsz, seqlen, d_model)\n",
        "    bsz, seqlen, d_model = x.shape\n",
        "    assert d_model == self.d_model\n",
        "    wqkv_out = self.wqkv(x) # (bsz, seqlen, 3*d_model)\n",
        "    # rearrange so seqlen,head_dim are the last two dims\n",
        "    wqkv_out = wqkv_out.reshape(bsz, 3, self.num_heads, seqlen, self.head_dim)\n",
        "    q = wqkv_out[:,0,:,:,:] # (bsz, num_heads, seqlen, head_dim)\n",
        "    k = wqkv_out[:,1,:,:,:] # (bsz, num_heads, seqlen, head_dim)\n",
        "    v = wqkv_out[:,2,:,:,:] # (bsz, num_heads, seqlen, head_dim)\n",
        "    # TODO: add rope to q and k\n",
        "    self_attn_out = self.scaled_dot_product_attn(q, k, v) #(bsz, num_heads, seqlen, head_dim)\n",
        "    # transpose\n",
        "    self_attn_out = self_attn_out.transpose(1,2) # (bsz,seqlen,num_heads,head_dim)\n",
        "    # concat the last dim\n",
        "    self_attn_out = self_attn_out.reshape(bsz,seqlen,-1)\n",
        "    # wo\n",
        "    self_attn_out = self_attn_out.reshape(bsz, seqlen, d_model)\n",
        "    wo_out = self.wo(self_attn_out)\n",
        "    return wo_out\n",
        "\n",
        "\n",
        "class TransformerBlock(torch.nn.Module):\n",
        "  def __init__(self, d_model, d_ffn, num_heads, max_seqlen):\n",
        "    super().__init__()\n",
        "    self.mlp_norm = torch.nn.RMSNorm(normalized_shape=d_model)\n",
        "    self.mlp = MLP(d_model=d_model, d_ffn=d_ffn)\n",
        "    self.attn_norm = torch.nn.RMSNorm(normalized_shape=d_model)\n",
        "    self.attn = MultiHeadAttention(d_model=d_model,num_heads=num_heads)\n",
        "\n",
        "\n",
        "  def forward(self, x:torch.Tensor):\n",
        "    # norm --> attn, add back to original --> norm --> mlp, add back\n",
        "    attn_norm_out = self.attn_norm(x)\n",
        "    print(f\"attn_norm_out: {attn_norm_out[0,0,:]}\")\n",
        "    attn_out = self.attn(attn_norm_out)\n",
        "    print(f\"attn_out: {attn_out[0,0,:]}\")\n",
        "    attn_out_post_resid = x + attn_out\n",
        "    mlp_norm_out = self.mlp_norm(attn_out_post_resid)\n",
        "    mlp_out = self.mlp(mlp_norm_out)\n",
        "    retval = mlp_out + attn_out_post_resid\n",
        "    return\n",
        "\n",
        "class Transformer(torch.nn.Module):\n",
        "  def __init__(self, d_model, d_ffn, vocab_size, num_layers, max_seqlen, num_heads, simple_pos_embed=False):\n",
        "    super().__init__()\n",
        "    self.max_seqlen = max_seqlen\n",
        "    self.embedding = torch.nn.Embedding(num_embeddings=vocab_size, embedding_dim=d_model)\n",
        "    torch.nn.init.trunc_normal_(self.embedding.weight, mean=0, std=1/d_model**0.5, a=-3, b=-3)\n",
        "\n",
        "    self.simple_pos_embed = simple_pos_embed\n",
        "    if simple_pos_embed:\n",
        "      self.pos_embedding = torch.nn.Embedding(num_embeddings=max_seqlen, embedding_dim=d_model)\n",
        "\n",
        "    self.output_layer = torch.nn.Linear(in_features=d_model, out_features=vocab_size)\n",
        "    torch.nn.init.trunc_normal_(self.output_layer.weight, mean=0, std=1/d_model**0.5, a=-3, b=3)\n",
        "    self.layers = torch.nn.ModuleList(\n",
        "        [TransformerBlock(d_model=d_model, d_ffn=d_ffn, num_heads=num_heads, max_seqlen=max_seqlen) for _ in range(num_layers)]\n",
        "    )\n",
        "    # TODO: do proper init\n",
        "    # add attention\n",
        "    # add layer norms\n",
        "\n",
        "  def forward(self, tokens: torch.Tensor):\n",
        "    # tokens: (bsz,seqlen,d_model)\n",
        "    seqlen = tokens.shape[1]\n",
        "    curr_out = self.embedding(tokens)\n",
        "    if self.simple_pos_embed:\n",
        "      curr_out = curr_out + self.pos_embedding(torch.arange(seqlen))\n",
        "    for l in self.layers:\n",
        "      curr_out = curr_out + l(curr_out)\n",
        "    logits = self.output_layer(curr_out)  # bsz,seqlen,vocab_size\n",
        "    return logits\n"
      ],
      "metadata": {
        "id": "KgGPaK6cERnA"
      },
      "execution_count": 253,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Test code"
      ],
      "metadata": {
        "id": "J3MNY6h3aQUh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tblock_test = TransformerBlock(d_model=D_MODEL, d_ffn=D_FFN, num_heads=NUM_HEADS, max_seqlen=SEQLEN)\n",
        "input_act = torch.randn(BSZ, SEQLEN, D_MODEL)\n",
        "tblock_out = tblock_test(input_act)\n",
        "assert tblock_out.shape == (BSZ, SEQLEN, D_MODEL)\n",
        "\n",
        "input_act_trunc = input_act[:,:2,:]\n",
        "tblock_out2 = tblock_test(input_act_trunc)\n",
        "\n",
        "print(input_act[0,0,:])\n",
        "print(input_act_trunc[0,0,:])\n",
        "\n",
        "print(tblock_out[0,0,:])\n",
        "print(tblock_out2[0,0,:])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 245
        },
        "id": "plQSbdRDZRDC",
        "outputId": "30dd9418-b82d-42ac-c15f-172ddf089010"
      },
      "execution_count": 254,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "attn_norm_out: tensor([ 0.452,  0.205, -1.575, -1.386, -1.095, -0.231,  1.232, -1.143,  0.256,  0.728, -0.624,  0.743,  1.186,  1.459,  1.153,  0.937], grad_fn=<SliceBackward0>)\n",
            "attn_out: tensor([ 0.075,  0.088, -0.319,  0.960,  1.338,  0.489, -0.344,  0.323,  1.019,  0.494,  0.272, -0.042,  0.438, -0.529,  0.839, -0.159], grad_fn=<SliceBackward0>)\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "'NoneType' object has no attribute 'shape'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3771345307.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0minput_act\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mBSZ\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSEQLEN\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mD_MODEL\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mtblock_out\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtblock_test\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_act\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0;32massert\u001b[0m \u001b[0mtblock_out\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mBSZ\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSEQLEN\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mD_MODEL\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0minput_act_trunc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput_act\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'shape'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Dataloader"
      ],
      "metadata": {
        "id": "tuQgoyYXaPL2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "EOS_TOKEN=-1\n",
        "\n",
        "class DigitDataset(IterableDataset):\n",
        "  def __init__(self, num_digits=10):\n",
        "    self.num_digits = 10\n",
        "\n",
        "  def __iter__(self):\n",
        "    while True:\n",
        "      for i in range(self.num_digits):\n",
        "        yield i\n",
        "\n",
        "\n",
        "def collate_batch_digits(batch, seqlen, bsz):\n",
        "  x = torch.tensor(batch, dtype=torch.long).view(bsz, seqlen)\n",
        "  y = torch.roll(x, shifts=-1, dims=(1,))\n",
        "  y[:,-1] = EOS_TOKEN\n",
        "  return x,y\n",
        "\n",
        "# dataset that spits out 8 unsorted digits, then sorts them all\n",
        "\n",
        "class SortedDigitsDataset(IterableDataset):\n",
        "  # outputs a full batch\n",
        "  def __init__(self, num_digits=10, max_seqlen=8, eos_token=EOS_TOKEN):\n",
        "    self.num_digits = 10\n",
        "    self.max_seqlen = max_seqlen\n",
        "    self.eos_token = eos_token\n",
        "\n",
        "  def __iter__(self):\n",
        "    while True:\n",
        "      half = int(self.max_seqlen//2)\n",
        "      half_batch = torch.randint(low=0,high=(self.num_digits-1), size=(half,))\n",
        "      sorted = torch.sort(half_batch).values\n",
        "      batch = torch.cat([half_batch, sorted])\n",
        "      targets = torch.roll(batch, shifts=-1)\n",
        "      targets[:int(half)-1] = self.eos_token\n",
        "      targets[-1] = self.eos_token\n",
        "      #print(f\"batch:{batch}\\n targets:{targets}\\n\")\n",
        "      yield (batch, targets)\n"
      ],
      "metadata": {
        "id": "OZPMJ9VHG2na"
      },
      "execution_count": 229,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Dataloader test"
      ],
      "metadata": {
        "id": "fhA1W-c9fuxK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sorted_digits_dataset = SortedDigitsDataset(max_seqlen=SEQLEN)\n",
        "sorted_dataloader = DataLoader(dataset=sorted_digits_dataset, batch_size=BSZ, shuffle=False)\n",
        "\n",
        "an_iter = iter(sorted_digits_dataset)\n",
        "while True:\n",
        "  print(next(an_iter))\n",
        "  break"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KByqwSM1fxyz",
        "outputId": "3b50007f-4d98-4e48-bbcf-72dfc3ed42e1"
      },
      "execution_count": 230,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(tensor([4, 8, 4, 4, 3, 1, 8, 8, 1, 3, 4, 4, 4, 8, 8, 8]), tensor([-1, -1, -1, -1, -1, -1, -1,  1,  3,  4,  4,  4,  8,  8,  8, -1]))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Constants"
      ],
      "metadata": {
        "id": "K6UBpVpR0w1T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "D_MODEL=16\n",
        "D_FFN=64\n",
        "NUM_LAYERS=2\n",
        "VOCAB_SIZE=16\n",
        "SEQLEN=16\n",
        "BSZ=32\n",
        "NUM_HEADS=4\n",
        "HEAD_DIM=D_MODEL//NUM_HEADS"
      ],
      "metadata": {
        "id": "iwsF3nnZ0wMK"
      },
      "execution_count": 231,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Dataloader"
      ],
      "metadata": {
        "id": "NR6aFw_S0z0P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sorted_digits_dataset = SortedDigitsDataset(max_seqlen=SEQLEN)\n",
        "sorted_dataloader = DataLoader(dataset=sorted_digits_dataset, batch_size=BSZ, shuffle=False)\n",
        "dl_iter = iter(sorted_dataloader)"
      ],
      "metadata": {
        "id": "isnsjgkh008a"
      },
      "execution_count": 232,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Trainer"
      ],
      "metadata": {
        "id": "Uxlg3n9kaR0O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "STEPS=5001\n",
        "\n",
        "model = Transformer(d_model=D_MODEL, d_ffn=D_FFN, vocab_size=VOCAB_SIZE, num_layers=NUM_LAYERS, num_heads=NUM_HEADS, max_seqlen=SEQLEN, simple_pos_embed=True)\n",
        "optimizer = torch.optim.AdamW(params=model.parameters(), lr=0.001, weight_decay=0)\n",
        "\n",
        "#digits = DigitDataset()\n",
        "#digit_dataloader = DataLoader(dataset=digits, batch_size=SEQLEN*BSZ, shuffle=False, collate_fn=lambda b:collate_batch_digits(batch=b, seqlen=SEQLEN, bsz=BSZ))\n",
        "#dl_iter = iter(digit_dataloader)\n",
        "\n",
        "\n",
        "for step in range(STEPS):\n",
        "  optimizer.zero_grad()\n",
        "  tokens, targets = next(dl_iter) #targets = (bsz, seqlen)   tokens=(bsz, seqlen)\n",
        "  logits = model(tokens)  # (bsz, seqlen, vocab_size)\n",
        "  vocab_size = logits.shape[-1]\n",
        "  loss = torch.nn.functional.cross_entropy(input=logits.view(-1, vocab_size), target=targets.view(-1), ignore_index=EOS_TOKEN)\n",
        "  if step %100 == 0:\n",
        "    print(f\"{step=} {loss=}\")\n",
        "    #print(f\"tokens:{tokens[0,:]} \\n targets:{targets[0,:]} \\n\\n\")\n",
        "    #print(f\"tokens:{tokens[0,:]} \\n targets:{targets[0,:]} \\n probs:\\n{logits[0,:,:]}\\n\")\n",
        "  loss.backward()\n",
        "  optimizer.step()\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ms1MEO7EGIDj",
        "outputId": "c7c651af-b2d6-4f36-a231-f86f0053a293"
      },
      "execution_count": 233,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/nn/init.py:295: UserWarning: mean is more than 2 std from [a, b] in nn.init.trunc_normal_. The distribution of values may be incorrect.\n",
            "  return _no_grad_trunc_normal_(tensor, mean, std, a, b, generator=generator)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step=0 loss=tensor(nan, grad_fn=<NllLossBackward0>)\n",
            "step=100 loss=tensor(nan, grad_fn=<NllLossBackward0>)\n",
            "step=200 loss=tensor(nan, grad_fn=<NllLossBackward0>)\n",
            "step=300 loss=tensor(nan, grad_fn=<NllLossBackward0>)\n",
            "step=400 loss=tensor(nan, grad_fn=<NllLossBackward0>)\n",
            "step=500 loss=tensor(nan, grad_fn=<NllLossBackward0>)\n",
            "step=600 loss=tensor(nan, grad_fn=<NllLossBackward0>)\n",
            "step=700 loss=tensor(nan, grad_fn=<NllLossBackward0>)\n",
            "step=800 loss=tensor(nan, grad_fn=<NllLossBackward0>)\n",
            "step=900 loss=tensor(nan, grad_fn=<NllLossBackward0>)\n",
            "step=1000 loss=tensor(nan, grad_fn=<NllLossBackward0>)\n",
            "step=1100 loss=tensor(nan, grad_fn=<NllLossBackward0>)\n",
            "step=1200 loss=tensor(nan, grad_fn=<NllLossBackward0>)\n",
            "step=1300 loss=tensor(nan, grad_fn=<NllLossBackward0>)\n",
            "step=1400 loss=tensor(nan, grad_fn=<NllLossBackward0>)\n",
            "step=1500 loss=tensor(nan, grad_fn=<NllLossBackward0>)\n",
            "step=1600 loss=tensor(nan, grad_fn=<NllLossBackward0>)\n",
            "step=1700 loss=tensor(nan, grad_fn=<NllLossBackward0>)\n",
            "step=1800 loss=tensor(nan, grad_fn=<NllLossBackward0>)\n",
            "step=1900 loss=tensor(nan, grad_fn=<NllLossBackward0>)\n",
            "step=2000 loss=tensor(nan, grad_fn=<NllLossBackward0>)\n",
            "step=2100 loss=tensor(nan, grad_fn=<NllLossBackward0>)\n",
            "step=2200 loss=tensor(nan, grad_fn=<NllLossBackward0>)\n",
            "step=2300 loss=tensor(nan, grad_fn=<NllLossBackward0>)\n",
            "step=2400 loss=tensor(nan, grad_fn=<NllLossBackward0>)\n",
            "step=2500 loss=tensor(nan, grad_fn=<NllLossBackward0>)\n",
            "step=2600 loss=tensor(nan, grad_fn=<NllLossBackward0>)\n",
            "step=2700 loss=tensor(nan, grad_fn=<NllLossBackward0>)\n",
            "step=2800 loss=tensor(nan, grad_fn=<NllLossBackward0>)\n",
            "step=2900 loss=tensor(nan, grad_fn=<NllLossBackward0>)\n",
            "step=3000 loss=tensor(nan, grad_fn=<NllLossBackward0>)\n",
            "step=3100 loss=tensor(nan, grad_fn=<NllLossBackward0>)\n",
            "step=3200 loss=tensor(nan, grad_fn=<NllLossBackward0>)\n",
            "step=3300 loss=tensor(nan, grad_fn=<NllLossBackward0>)\n",
            "step=3400 loss=tensor(nan, grad_fn=<NllLossBackward0>)\n",
            "step=3500 loss=tensor(nan, grad_fn=<NllLossBackward0>)\n",
            "step=3600 loss=tensor(nan, grad_fn=<NllLossBackward0>)\n",
            "step=3700 loss=tensor(nan, grad_fn=<NllLossBackward0>)\n",
            "step=3800 loss=tensor(nan, grad_fn=<NllLossBackward0>)\n",
            "step=3900 loss=tensor(nan, grad_fn=<NllLossBackward0>)\n",
            "step=4000 loss=tensor(nan, grad_fn=<NllLossBackward0>)\n",
            "step=4100 loss=tensor(nan, grad_fn=<NllLossBackward0>)\n",
            "step=4200 loss=tensor(nan, grad_fn=<NllLossBackward0>)\n",
            "step=4300 loss=tensor(nan, grad_fn=<NllLossBackward0>)\n",
            "step=4400 loss=tensor(nan, grad_fn=<NllLossBackward0>)\n",
            "step=4500 loss=tensor(nan, grad_fn=<NllLossBackward0>)\n",
            "step=4600 loss=tensor(nan, grad_fn=<NllLossBackward0>)\n",
            "step=4700 loss=tensor(nan, grad_fn=<NllLossBackward0>)\n",
            "step=4800 loss=tensor(nan, grad_fn=<NllLossBackward0>)\n",
            "step=4900 loss=tensor(nan, grad_fn=<NllLossBackward0>)\n",
            "step=5000 loss=tensor(nan, grad_fn=<NllLossBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# generation\n",
        "# dumb way: 1 token at a time. get logits, do softmax, sample token. feed in full generated thing back\n",
        "\n",
        "# kv cache way: attention takes in a kv cache which are pre-computed k and v values for previous tokens\n",
        "\n"
      ],
      "metadata": {
        "id": "0FbHrEmhMayx"
      },
      "execution_count": 171,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Simple generation"
      ],
      "metadata": {
        "id": "TYAurIkmqA74"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def softmax_with_temp_last_dim(logits: torch.Tensor, temp: float=1.0, eps=1e-6):\n",
        "  # softmax across the innermost (last) dimension\n",
        "  # compute max\n",
        "  # subtract max\n",
        "  # numerator = exponentiate with temperature\n",
        "  # denominator = sum of numerator\n",
        "  #import pdb; pdb.set_trace()\n",
        "  # logits: (..., vocab_size)\n",
        "  maxval = torch.amax(logits, dim=-1).unsqueeze(-1)\n",
        "  max_subtracted_logits = logits - maxval\n",
        "  scaled_max_subtracted_logits = max_subtracted_logits/torch.Tensor([temp+eps])\n",
        "  numerator = torch.exp(scaled_max_subtracted_logits)\n",
        "  denominator = torch.sum(numerator, dim=-1).unsqueeze(-1)\n",
        "  return numerator/(denominator + eps)\n",
        "\n",
        "\n",
        "def retain_top_p(probs: torch.Tensor, p: float):\n",
        "  # retain only probabilities with mass = top_p\n",
        "  assert len(probs.shape) == 1 # batch later\n",
        "  sorted_probs = torch.sort(probs, descending=True)\n",
        "  sorted_probs_values = sorted_probs.values\n",
        "  sorted_probs_indices = sorted_probs.indices\n",
        "  cumsum_probs = list(torch.cumsum(sorted_probs_values, dim=0))\n",
        "  for i, prob in enumerate(cumsum_probs):  # super ugly, need vectorized version\n",
        "    if prob > p:\n",
        "      break\n",
        "  not_useful_indices = sorted_probs_indices[(i+1):]\n",
        "  if len(not_useful_indices) > 0:\n",
        "    probs[not_useful_indices] = 0 # TODO check here\n",
        "  return probs\n",
        "\n",
        "\n",
        "\n",
        "def sample(logits: torch.Tensor, top_p: Optional[float]=None, temp: float = 1.0):\n",
        "  # logits: (vocab_size,)\n",
        "  probs = softmax_with_temp_last_dim(logits=logits, temp=temp)\n",
        "  print(probs)\n",
        "  if top_p:\n",
        "    probs = retain_top_p(probs=probs, p=top_p)\n",
        "  sampled = np.random.multinomial(n=1, pvals=probs)\n",
        "  return int(np.argwhere(sampled)[0][0])\n",
        "\n",
        "\n",
        "def generate(the_model: torch.nn.Module, input_tokens: torch.Tensor, toks_to_generate: int, temp=1.0):\n",
        "  # for each token, run forward\n",
        "    # run forward get logits\n",
        "    # convert logits to probabilities\n",
        "    # sample from probabilities\n",
        "    # update input tokens\n",
        "  generated_toks = []\n",
        "  for _ in range(toks_to_generate):\n",
        "    logits = the_model(input_tokens)\n",
        "    #print(f\"logits.shape:{logits.shape}\")\n",
        "    print(f\"full_logits:{logits}\")\n",
        "    logits = logits.squeeze(0)[-1,:] #unsqueeze to remove batch dim. -1 for last token\n",
        "    print(f\"logits:{logits}\")\n",
        "    sampled_token = sample(logits, temp=temp)\n",
        "    generated_toks.append(sampled_token)\n",
        "    #print(input_tokens.shape)\n",
        "    #print(torch.tensor([sampled_token]).unsqueeze(0).shape)\n",
        "    input_tokens = torch.cat([input_tokens.squeeze(0), torch.tensor([sampled_token])]).unsqueeze(0)\n",
        "  return generated_toks\n",
        ""
      ],
      "metadata": {
        "id": "rtQTtV_sqCSL"
      },
      "execution_count": 219,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Simple tests"
      ],
      "metadata": {
        "id": "PVQertKT70yf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "input, targets = next(dl_iter) #get the first entry only\n",
        "input = input[:1,:]\n",
        "targets = targets[:1,:]\n",
        "print(input)\n",
        "print(targets)\n",
        "trunc_input = input[:1,:int(SEQLEN//2)]\n",
        "print(trunc_input)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nlqqXCBI51Wy",
        "outputId": "0fe9cc8d-bb34-4a55-bbe5-c94ca68750d8"
      },
      "execution_count": 224,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[8, 7, 5, 4, 7, 4, 4, 1, 1, 4, 4, 4, 5, 7, 7, 8]])\n",
            "tensor([[-1, -1, -1, -1, -1, -1, -1,  1,  4,  4,  4,  5,  7,  7,  8, -1]])\n",
            "tensor([[8, 7, 5, 4, 7, 4, 4, 1]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "logits_raw = model(input)\n",
        "print(f\"raw_logits:{logits_raw}\")\n",
        "print(f\"logits for last:{logits_raw.squeeze(0)[7,:]}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tLKWEyqo6ydq",
        "outputId": "1efbd375-5efd-44bf-8c5f-bb575e30493e"
      },
      "execution_count": 225,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "raw_logits:tensor([[[-8.717e+00,  3.113e+00, -3.977e+01, -9.374e+00,  2.456e+01,  3.480e+01,  8.431e+00,  3.421e+00,  4.940e+01, -3.145e+01, -2.733e+01, -1.041e+01, -1.330e+01, -4.251e+00, -5.668e+00, -1.252e+01],\n",
            "         [ 1.035e+01,  8.766e-01, -3.859e+01, -1.126e+01,  3.591e+01,  3.384e+01, -6.838e+00, -1.821e+01,  3.177e+01, -2.001e+01, -1.713e+01, -9.427e+00, -1.330e+01, -4.967e+00, -6.192e+00, -2.626e+00],\n",
            "         [-1.887e+01,  1.556e+01,  5.055e-01, -1.758e+01, -2.055e+01,  7.803e+00,  2.347e+01,  1.953e+01,  2.444e+00, -2.635e+01, -1.841e+01,  1.042e+00,  3.919e-01, -1.387e+01, -5.886e+00,  8.695e-01],\n",
            "         [-4.570e+00,  1.598e+01, -7.507e+00,  9.737e+00,  1.143e+01,  8.296e+00, -1.097e+01, -5.354e+00,  9.391e+00, -2.693e+01, -1.223e+01, -8.187e+00, -7.584e-01, -8.673e+00, -1.328e+01, -2.021e+01],\n",
            "         [ 4.740e+00,  2.614e+01, -2.655e+01,  7.097e+00, -1.026e+01, -8.144e+00,  1.667e+01,  2.233e+01,  1.499e+01, -2.901e+01, -1.381e+01, -1.920e+01, -3.452e+01, -1.587e+01, -2.893e+01, -3.936e+01],\n",
            "         [ 3.518e+01, -2.680e+00, -4.787e+01,  1.651e+00,  3.396e+01,  1.067e+01, -1.154e+01, -5.767e+00,  2.843e+01, -3.419e+00, -9.925e+00, -1.860e+01, -2.862e+01,  2.446e+00, -1.384e+01, -2.250e+01],\n",
            "         [ 1.182e+01,  9.380e+00, -3.708e+01, -6.749e-01,  1.401e+01,  4.258e+00, -4.805e-01,  1.842e+01,  2.506e+01, -1.544e+01, -1.837e+01, -1.155e+01, -2.382e+01, -1.628e+01, -1.874e+01, -2.113e+01],\n",
            "         [ 1.063e+01,  3.110e+01, -3.090e+01,  1.851e+00,  1.518e+01,  1.463e+01, -1.698e+00,  7.571e-01,  9.948e+00, -3.637e+01, -2.222e+01, -1.926e+01, -2.372e+01, -2.471e+01, -2.612e+01, -3.522e+01],\n",
            "         [ 1.340e+01,  1.438e+01, -3.228e+01,  3.108e+00,  2.098e+01,  1.496e+01, -4.159e-01,  7.200e+00,  1.524e+01, -1.849e+01, -1.255e+01, -1.191e+01, -2.178e+01, -1.529e+01, -2.182e+01, -2.708e+01],\n",
            "         [ 2.480e+01, -5.184e+00, -4.440e+01, -9.710e+00,  3.890e+01,  3.056e+01, -4.695e+00, -8.780e+00,  2.037e+01, -1.610e+01, -1.410e+01, -5.678e+00, -1.411e+01, -7.386e+00, -1.172e+01, -1.076e+01],\n",
            "         [ 1.192e+01,  1.559e+00, -2.563e+01, -7.503e+00,  2.405e+01,  1.474e+01, -7.586e+00,  7.978e-01,  1.410e+01, -4.911e+00, -1.651e+01, -9.407e+00, -7.437e+00, -7.208e+00, -6.904e+00, -7.098e+00],\n",
            "         [-1.758e+00,  7.362e+00, -2.015e+01, -1.789e+01,  1.047e+01,  2.299e+01,  9.297e+00,  7.289e+00,  1.260e+01, -9.755e+00, -1.720e+01, -7.157e-01, -2.341e+00, -1.172e+01, -4.044e-01,  1.984e+00],\n",
            "         [-5.475e+00,  1.004e+01, -6.584e+00, -1.357e+01, -1.061e+01,  2.841e+00,  1.174e+01,  2.112e+01,  1.463e+01, -1.450e+01, -1.902e+01,  4.422e+00, -7.515e+00, -4.187e+00, -1.471e+00,  2.202e+00],\n",
            "         [-1.138e+01,  4.416e+00,  1.739e+00,  2.517e+00, -1.624e+01, -1.309e+01,  1.627e+00,  2.759e+01,  1.025e+00, -8.001e+00, -1.322e+01,  4.955e+00, -7.510e+00, -1.020e+01, -1.090e+01, -6.167e+00],\n",
            "         [ 5.374e+00,  4.001e+00, -2.688e+01,  3.687e+00,  1.539e+01,  2.791e-02, -1.393e+01,  1.008e+01,  2.970e+01, -6.716e+00, -2.044e+01, -7.192e+00, -2.118e+01, -2.369e+00, -9.790e+00, -1.234e+01],\n",
            "         [-2.210e+01,  9.184e-01, -2.036e+01,  5.987e+00,  2.624e+01,  2.789e+01, -6.522e+00, -1.882e+01,  2.071e+01, -2.448e+01, -1.096e+01, -1.201e+01, -3.773e+00, -1.743e+01, -1.192e+01, -9.586e+00]]], grad_fn=<ViewBackward0>)\n",
            "logits for last:tensor([ 10.629,  31.095, -30.903,   1.851,  15.176,  14.628,  -1.698,   0.757,   9.948, -36.375, -22.220, -19.257, -23.725, -24.710, -26.125, -35.220], grad_fn=<SliceBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# test for sampling functions\n",
        "#logits = torch.tensor([5,1,1,5,1])\n",
        "#sample(logits, temp=0)\n",
        "\n",
        "with torch.no_grad():\n",
        "  # test for full generation\n",
        "  #input = next(dl_iter)[0][0,:].unsqueeze(0) #get the first entry only\n",
        "  generated = generate(the_model=model, input_tokens=trunc_input, toks_to_generate=1, temp=1)\n",
        "  print(generated)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-shFNf4v7z5A",
        "outputId": "27445435-c30c-48c4-fb3b-922e2e18ea7b"
      },
      "execution_count": 226,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "full_logits:tensor([[[ -0.638,  19.998, -19.026,   3.030,  -0.644,   4.044,   4.200,  18.733,  26.821, -27.488, -19.970,  -7.010, -19.914,  -5.486, -11.259, -21.677],\n",
            "         [ 16.976,   4.150, -41.736,  -7.956,  35.129,  31.589,  -8.484, -10.738,  33.263, -25.014, -20.447,  -9.452, -18.661,  -6.777, -10.125, -10.792],\n",
            "         [ 30.958,   6.546, -40.985, -13.567,  33.140,  21.235,  -8.875,  -2.774,  31.594, -10.831, -22.016, -11.067, -18.855,   2.142,  -5.156,  -9.298],\n",
            "         [ -3.273,   9.484,  -7.698,  -5.889,  27.474,  28.073,  -9.269, -21.469,   5.784, -22.441, -17.575, -11.469,  11.179,  -5.281,  -3.860,  -7.326],\n",
            "         [ 10.631,  -0.351, -38.579,  -6.840,  34.729,  25.680, -12.670, -10.089,  36.073,  -7.165, -19.532, -10.590, -17.231,  -2.687,  -2.040,  -4.257],\n",
            "         [ -4.842,  11.866,  -7.065,  -0.214,  -3.651,   8.947,  12.381,  -4.340,  -5.157, -19.088,  -4.776, -14.768,  -8.145, -12.156, -12.376, -17.710],\n",
            "         [ 26.398,  -2.742, -41.691,  -4.501,  52.266,  24.509, -27.102,  -7.958,  38.564,  -6.163, -24.552, -13.517, -15.057,  -4.952,  -4.531,  -6.751],\n",
            "         [  6.205,  15.997, -22.640,  -0.580,  23.006,  25.911,  -5.313, -12.342,   9.431, -24.128, -14.497, -14.301, -10.960, -16.343, -12.033, -19.120]]])\n",
            "logits:tensor([  6.205,  15.997, -22.640,  -0.580,  23.006,  25.911,  -5.313, -12.342,   9.431, -24.128, -14.497, -14.301, -10.960, -16.343, -12.033, -19.120])\n",
            "tensor([2.623e-09, 4.690e-05, 7.791e-22, 2.966e-12, 5.192e-02, 9.480e-01, 2.610e-14, 2.312e-17, 6.603e-08, 1.759e-22, 2.680e-18, 3.257e-18, 9.202e-17, 4.227e-19, 3.147e-17, 2.631e-20])\n",
            "[5]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Sample"
      ],
      "metadata": {
        "id": "CVPCOS8kEa2R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "probs"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YDpViMId8pqe",
        "outputId": "96410a6e-67b4-40cd-8b34-908d8c9fe63e"
      },
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([0.487, 0.009, 0.009, 0.487, 0.009])"
            ]
          },
          "metadata": {},
          "execution_count": 62
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "int(np.argwhere(np.random.multinomial(n=1, pvals=probs))[0][0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sI8RXUbQBFTY",
        "outputId": "ce77aa01-90aa-4dab-d7d4-09b077f10981"
      },
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "3"
            ]
          },
          "metadata": {},
          "execution_count": 57
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KQDNdkfOCUfy",
        "outputId": "9aa07fef-7735-422c-81d1-46b72a2614ce"
      },
      "execution_count": 133,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Transformer(\n",
              "  (embedding): Embedding(16, 16)\n",
              "  (pos_embedding): Embedding(16, 16)\n",
              "  (output_layer): Linear(in_features=16, out_features=16, bias=True)\n",
              "  (layers): ModuleList(\n",
              "    (0-1): 2 x TransformerBlock(\n",
              "      (mlp_norm): RMSNorm((16,), eps=None, elementwise_affine=True)\n",
              "      (mlp): MLP(\n",
              "        (fc1): Linear(in_features=16, out_features=64, bias=False)\n",
              "        (fc2): Linear(in_features=64, out_features=16, bias=False)\n",
              "      )\n",
              "      (attn_norm): RMSNorm((16,), eps=None, elementwise_affine=True)\n",
              "      (attn): MultiHeadAttention(\n",
              "        (wqkv): Linear(in_features=16, out_features=48, bias=False)\n",
              "        (wo): Linear(in_features=16, out_features=16, bias=True)\n",
              "      )\n",
              "    )\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 133
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "aLwixnft04Bb"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}